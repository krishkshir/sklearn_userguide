# Supervised learning

## Generalized linear models

* An sklearn model can be saved using Python's pickle module, but it's more
  efficient to use joblib’s replacement for pickle (joblib.dump & joblib.load),
  which is more efficient on big data but it can only pickle to the disk
  and not to a string:
```
>>> from joblib import dump, load
>>> dump(clf, 'filename.joblib')
>>> clf2 = load('filename.joblib')
```
* In sklearn, unless otherwise specified, inputs and regression targets are cast
 to float64, whereas classification targets maintain their dtypes.
* Hyper-parameters of an estimator can be updated after it has been constructed
  via the `set_params()` method.
* Calling fit() more than once will overwrite what was learned by any previous
  fit().
* For multiclass classification (i.e., when output is a categorical variable
  with more than 2 possible values), one can use
  `sklearn.preprocessing.LabelBinarizer` that essentially converts the label into a
  one-hot encoded version.
```
>>> from sklearn import preprocessing
>>> lb = preprocessing.LabelBinarizer()
>>> lb.fit([1, 2, 6, 4, 2])
LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)
>>> lb.classes_
array([1, 2, 4, 6])
>>> lb.transform([1, 6])
array([[1, 0, 0, 0],
       [0, 0, 0, 1]])
```
    - For binary classes, the output is transformed to single column vector
    ```
    >>> lb = preprocessing.LabelBinarizer()
>>> lb.fit_transform(['yes', 'no', 'no', 'yes'])
array([[1],
       [0],
       [0],
       [1]])
    ```
* For multilabel outputs (i.e., when the output can be assigned multiple labels
  simultaneously and not necessarily the same number of labels in each case),
  one can use `sklearn.preprocessing.MultiLabelBinarizer` that (similar to
  LabelBinarizer) provides a 1-hot encoded version of the labels.
* In sklearn estimators expose a score & the convention is: bigger score is better
* OLS has complexity `O(n_samples*n_features^2)` assuming `n_samples >= n_features`
* RidgeCV, by default, implements Generalized CV (a more efficient version of
  leave-one-out cross-validation involving an analytical formula for computing
  the optimum value of the regularization parameter see http://pages.stat.wisc.edu/~wahba/ftp1/oldie/craven.wah.pdf), but setting the cv parameter to an integer
  makes it switch to GridSearchCV
* For high-dimensional datasets with many collinear features, LassoCV is most often preferable. However, LassoLarsCV has the advantage of exploring more relevant values of alpha parameter, and if the number of samples is very small compared to the number of features, it is often faster than LassoCV.
* Alternatively, the estimator LassoLarsIC proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to find the optimal value of alpha as the regularization path is computed only once instead of k+1 times when using k-fold cross-validation. However, such criteria needs a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples)
* The MultiTaskLasso is a linear model that estimates sparse coefficients for multiple regression problems jointly: y is a 2D array, of shape `(n_samples, n_tasks)`. The constraint is that the selected features are the same for all the regression problems, also called tasks.
* Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.
* Least-angle regression (LARS) is a regression algorithm for high-dimensional data,  similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.
* `OrthogonalMatchingPursuit` and `orthogonal_mp` implements the OMP algorithm for
  approximating the fit of a linear model with constraints imposed on the number
  of non-zero coefficients (ie. the `l_0` pseudo-norm).
* Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.
* The l2 regularization of ridge regression can be thought of as the MAP
  estimate under a Gaussian prior over the coefficients with precision equal to
  reciprocal of the regularization hyperparameter.
* Bayesian Ridge Regression is used for regression by the class BayesianRidge
  and is more robust to ill-posed problems.
* ARDRegression (Automatic Relevance Detection) is similar to Lasso regression,
  in that  it leads to sparser coeffs.
  It does this by posing a different prior over the coeffs w, instead of
  spherical Gaussians, it assumes axis-parallel, elliptical Gaussian
  distributions, i.e., each `w_i` has its own s.d.
* sklearn's LogisticRegression class can fit binary, one-vs-rest, or multinomial
  logistic regression with optional l1, l2 or elastic-net regularization.
  Note that, l2 regularization is applied by default.
* The `partial_fit` method allows online/out-of-core learning.
* The classes SGDClassifier and SGDRegressor provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with loss="log", SGDClassifier fits a logistic regression model, while with loss="hinge" it fits a linear support vector machine (SVM).
* The Perceptron is another simple classification algorithm suitable for large scale learning. By default:
    - It does not require a learning rate.
    - It is not regularized (penalized).
    - It updates its model only on mistakes.
The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.
* The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C.
For classification, PassiveAggressiveClassifier can be used with loss='hinge' (PA-I) or `loss='squared_hinge'` (PA-II). For regression, PassiveAggressiveRegressor can be used with `loss='epsilon_insensitive'` (PA-I) or `loss='squared_epsilon_insensitive'` (PA-II).
* Scikit-learn provides 3 robust regression estimators, i.e., regression in the
  presence of corrupt data due to outliers or errors: RANSAC, Theil Sen and
  HuberRegressor. When in doubt, use RANSAC (Random Sample Consensus), which
  fits a model from random subsets of inliers from the complete data set.
* The PolynomialFeatures transformer transforms an input data matrix into a new data matrix of a given degree.
    - In some cases it’s not necessary to include higher powers of any single feature, but only the so-called interaction features that multiply together at most  distinct features. These can be obtained from PolynomialFeatures with the setting `interaction_only=True`.

## Linear and Quadratic Discriminant analysis

* `discriminant_analysis.LinearDiscriminantAnalysis` and
  `discriminant_analysis.QuadraticDiscriminantAnalysis` are 2 classic classifiers
  that learn a linear & quadratic decision surface respectively and have the
  foll. advantages:
    - have easily computable closed-form solutions
    - are inherently multi-class
    - no hyperparameters to tune
    - proven to work well in practice
* LDA is a classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.
The model fits a Gaussian density to each class, assuming that all classes share
the same covariance matrix.
* QDA is a classifier with a quadratic decision boundary, generated by fitting
  class conditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class, without assuming that each class has the same covariance matrix.
* Mathematical formulation:
```
P(y=k|X) = \frac{P(X|y=k) P(y=k)}{\sum_l P(X|y=l) P(y=l)}
```
and selecting the class k which maximizes the P value and where
```
P(X|y=k) = \frac{1}{(2 \pi)^d/2 \Sigma_k^{1/2}} \exp(-1/2 (X-\mu_k)^T
\Sigma_k^{-1} (X-\mu_k))
```
where d is the number of features, P(y=k) is simply the proportion of instances in
class k, class means `\mu_k`, the empirical sample class means & the covariance
matrices `\Sigma_k`, either as the empirical sample covariances or by a
regularized estimator.
    - In LDA, the Gaussians for each class are assumed to share the same
      covariance matrix: `\Sigma_k = \Sigma` for all k, which leads to a linear
      decision boundary as can be seen by comparing log probability ratios.
    - In QDA, there are no such assumptions on `\Sigma_k`, leading to quadratic
      decision surfaces
        * If we assume the covariance matrices are diagnoal, we end up with
          Gaussian Naive Bayes classifier naive_bayes.GaussianNB, which assumes
          that inputs are conditionally independent in each class
* `discriminant_analysis.LinearDiscriminantAnalysis` can be used to perform
  supervised dimensionality reduction by using the transform method with the
  desired no. of compnents set using the `n_components` constructor parameter.
    - This parameter has no effect on .fit() & .predict()
    - Essentially, we're projecting the input data to a linear subspace
    consisting of the directions which maximize the separation between classes,
    defined in terms of the centroid locaitons.
* Shrinkage is a tool to improve estimation of covariance matrices in situations
  where the number of training samples is small compared to the number of
  features, in which case the empirical sample  covariance is a poor estimator.
    - can be used by setting the shrinkage parameter of the `discriminant_analysis.LinearDiscriminantAnalysis` class to ‘auto’.
        The shrinkage parameter can also be manually set between 0 and 1. In
          particular, a value of 0 corresponds to no shrinkage (which means the
          empirical covariance matrix will be used) and a value of 1 corresponds
          to complete shrinkage (which means that the diagonal matrix of
          variances will be used as an estimate for the covariance matrix).

## Kernel ridge regression

* KRR combines ridge regression (linear regression with l2 regularization)  with
  the kernel trick.
    - In effect, it learns a linear function in the kernel space, which implies
      for a non-linear kernel, it learns a non-linear function in feature space.
* The form of the model learned by KRR is identical to support vector regression
  (SVR), but it uses squared error loss, whereas the latter uses `\epsilon`-
  insensitive loss (but both have an l2 regularization term)
* In contrast to SVR, fitting KRR can be done in closed-form and is faster for
  medium-sized datasets (but doesn't scale well for large datasets),
  but since the resulting model is non-sparse, prediction is much slower than SVR

## Support vector machines

* SVM's are effecrive in high dimensional space, even if the no. of features is
  greater than the no. of samples, but must use regularization for avoiding
  over-fitting
* Since it uses only a subset of training points in the decision function, so it
  also memory efficient
* Since SVM don't provide probability estimates directly, these must be
  calculated using an expensive cross-validation.
* To use an SVM to make predictions for sparse data, it must have been fit on
  such data.
* SVC, NuSVC and LinearSVC are classes capable of performing multi-class classification on a dataset.
* SVC & NuSVC are mathematically equivalent, but NuSVC allows one to set an
  upper bound on the fraction of training errors, which is the same as the lower
  bound on the fraction of support vectors.
* LinearSVC is another implementation of SVC based on liblinear library instead
  of the libsvm library of SVC, that scales better with training data set size.
    - Also, for multi-class classification, it implements a one-vs-rest strategy
      instead of the one-vs-one implemented by the other SVC's
* The decision_function method of SVC and NuSVC gives per-class scores for each
  sample (or a single score per sample in the binary case). When the constructor
  option probability is set to True, class membership probability estimates
  (from the methods predict_proba and predict_log_proba) are enabled. In the
  binary case, the probabilities are calibrated using Platt scaling: logistic
  regression on the SVM’s scores, fit by an additional cross-validation on the
  training data, which is known to have theoretical issues, so if confidence
  scores are required, but these do not have to be probabilities, then it is
  advisable to set probability=False and use decision_function instead of predict_proba.
* To give more weight to certain classes, you can use parameter class_weight in
  SVC, but not in NuSVC, e.g., class_weight='balanced' will scale class weights
  according to the no. of examples.
* the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.
* There are three different implementations of Support Vector Regression: SVR, NuSVR and LinearSVR. LinearSVR provides a faster implementation than SVR but only considers linear kernels
* The class OneClassSVM implements a One-Class SVM which is used in outlier detection.
* The core of an SVM is a quadratic programming problem (QP), separating support
  vectors from the rest of the training data implemented by libsvm which scales
  between O(n_features * n_samples^2) & O(n_features * n_samples^3)
* By default, the kernel functions available are:
    - linear
    - poly
    - rbf
    - sigmoid
    - can also pass the gram matrix (np.dot(X, X.T)) and set kernel to
      'precomputed'
* For rbf kernel, The larger gamma is, the closer other examples must be to be
  affected. It is recommended to use GridSearchCV to find the best combination
  of C & gamma, since it's critical to performance of rbf SVC

## Stochastic Gradient Descent

* When using SGDClassifier, make sure you permute (shuffle) your training data
  before fitting the model or use shuffle=True to shuffle after each iteration.
* To get the signed distance to the hyperplane use
  SGDClassifier.decision_function.
* SGDClassifier can use the following loss functions:
    - ‘hinge’ (soft-margin linear SVM)
    - ‘log’ (logistic regression)
    - ‘modified_huber’ (smoothed hinge loss)
    - ‘squared_hinge’ (like hinge but quadratically penalized)
    - ‘perceptron’ (linear loss used by perceptron algorithm, i.e. ReLU)
    - a regression loss:
        * ‘squared_loss’ (OLS)
        * ‘huber’ (Huber loss for robust regression)
        * ‘epsilon_insensitive’ (linear SVR, ignores errors less than epsilon
          linear after that)
        * ‘squared_epsilon_insensitive’ (same as above with squared loss instead
          of linear)
    - hinge & log loss functions are lazy, i.e., update the model parameters
      only if the training example violates the margin constraint, making
      training very efficient & the model sparse, even with L2 penalty and they
      also enable the `predict_proba` method, which gives a vector of
      probability estimates P(y|x) per sample x
* SGD supports the foll. penalty parameters:
    - penalty="l2": L2 norm penalty on `coef_`
    - `- penalty="l1": L1 norm penalty on coef_`
    - penalty = "elasticnet": `penalty = (1-l1_ratio)*L2 + l1_ratio*L1`
* SGDClassifier supports multi-class classification by combining
  binary classifiers in a one-vs-all (OVA) scheme.
    - At test time, the confidence score (i.e., signed distance to the
      hyperplane) for each classifier is computed and the label is assigned to
      be the class with the highest confidence.
* SGDClassifier supports both weighted classes and weighted instances via the
  fit parameters `class_weight` and `sample_weight`.
* SGDClassifier supports averaged SGD (ASGD). Averaging can be enabled by setting `average=True`. ASGD works by averaging the coefficients of the plain SGD over each iteration over a sample. When using ASGD the learning rate can be larger and even constant leading on some datasets to a speed up in training time.
* The class SGDRegressor implements a plain stochastic gradient descent learning
  routine which supports different loss functions and penalties to fit linear
  regression models. SGDRegressor is well suited for regression problems with a
  large number of training samples (> 10.000).
    - As before, SGDRegressor supports averaged SGD as SGDClassifier. Averaging can be enabled by setting `average=True`
* The classes SGDClassifier and SGDRegressor provide two criteria to stop the algorithm when a given level of convergence is reached:
    - With early_stopping=True, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score computed on the validation set. The size of the validation set can be changed with the parameter validation_fraction.
    - With early_stopping=False, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the input data.
  In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve n_iter_no_change times in a row. The improvement is evaluated with a tolerance tol, and the algorithm stops in any case after a maximum number of iteration max_iter.
* Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. 
* If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant c such that the average L2 norm of the training data equals one.
* In contrast to (batch) gradient descent, SGD approximates the true gradient of  by considering a single training example at a time.
* For classification, the default learning rate schedule is inverse of iteration
  time (i.e. 1/(t_0 + t)), whereas for regression, it's exponentially decreasing
  (i.e., eta0/t^pow_t)

## Nearest neighbors

* `sklearn.neighbors` provide a set of what are known as "non-generalizable" ML
  models based on finding a predefined number of training samples closest in
  distance (usu. Euclidean distance) and making predictions based on these, so
  called because these algorithms simply memorize all the training data (or
  maybe transformed to a fast indexing structure such as Ball Tree or KD-tree).
    - Being a non-parametric method, nearest neighbors is very successful when
      the decision boundary is very irregular.
    - kernel density estimation relies on nearest neighbors at its core.
* `NearestNeighbors` implements unsupervised nearest neighbors learning and acts
  as a uniform interface to 3 different NN algos set by the `algorithm`
  parameter:
    - BallTree 
    - KDTree
    - brute-force algorithm based on routines in `sklearn.metrics.pairwise`
* If 2 neighbors are equidistant but different labels, the result will depend on
  the ordering of the training data.
* To efficiently produce a sparse graph showing the connections between
  neighboring points, use `kneighbors_graph()` method.
    - The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors.
* RadiusNeighborsClassifier implements learning based on the number of neighbors
  within a fixed radius r of each training point, where r is a floating-point
  value specified by the user. This is especially useful when the data is not
  uniformly spaced, but it becomes less effective in high-dimensional spaces due
  to the "curse of dimensionality".
* KNeighborsClassifier implements the basic kNN algo, but can be modified to
  assign weights to each neighbor by using the `weights=distance` parameter,
  alt. a user-defined function of distance can also be used.
* sklearn implements two NN regressions, where the label is predicted from the
  mean of the labels of the nearest neighbors: KNeighborsRegressor &
  RadiusNeighborsRegressor, similar to the classifiers and support similar
  parameters.
* Brute-force computation of distances between all pairs of training points in D
  dimensions scales as O(DN^2)
* The basic idea of KD-trees is that if point A is very distant from point B and
  point C is close to point B, then we can assert that point A is very distant
  from point C without computing the distance, so by storing the points in a
  tree structure (with data points partitioned along the D axes) leading to a
  computational complexity O(DNlogN) for small N and O(DN^2) for large D
    - very inefficient for high D, due to curse of dimensionality
* Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions.
    - A ball tree recursively divides the data into nodes defined by a centroid
      C and radius r, such that each point in the node lies within the
      hyper-sphere defined byC and r. The number of candidate points for a neighbor search is reduced through use of the triangle inequality
    - With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a KD-tree in high dimensions, though the actual performance is highly dependent on the structure of the training data.
* While ball tree complexity grows approximately as O(DNlogN)
* For KD trees & Ball trees, In general, sparser data with a smaller intrinsic dimensionality leads to faster query times. Because the KD tree internal representation is aligned with the parameter axes, it will not generally show as much improvement as ball tree for arbitrarily structured data.
* Ball tree and KD tree query time will become slower as k increases. This is due to two effects: first, a larger k leads to the necessity to search a larger portion of the parameter space. Second, using k > 1 requires internal queueing of results as the tree is traversed.
* The `leaf_size` parameter in Ball & KD trees determines at what level brute
  search would be applied
* The NearestCentroid classifier is a simple algorithm that represents each class
by the centroid of its members, in effect making it similar to the labeling
phase of KMeans.
    - Since it doesn't have any hyperparameters to tune, it's a good baseline
      classifier.
    - It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed
    - can use the `shrink_threshold` parameter to reduce the effect of noise
* Neighborhood Components Analysis (NCA, NeighborhoodComponentsAnalysis) is a distance metric learning algorithm which aims to improve the accuracy of nearest neighbors classification compared to the standard Euclidean distance. The algorithm directly maximizes a stochastic variant of the leave-one-out k-nearest neighbors (KNN) score on the training set. It can also learn a low-dimensional linear projection of data that can be used for data visualization and fast classification.
    - Combined with a nearest neighbors classifier (KNeighborsClassifier), NCA is attractive for classification because it can naturally handle multi-class problems without any increase in the model size, and does not introduce additional parameters that require fine-tuning by the user.
    - NCA classification has been shown to work well in practice for data sets of varying size and difficulty. In contrast to related methods such as Linear Discriminant Analysis, NCA does not make any assumptions about the class distributions. The nearest neighbor classification can naturally produce highly irregular decision boundaries.
    - NCA can be used to perform supervised dimensionality reduction. The input
      data are projected onto a linear subspace consisting of the directions
      which minimize the NCA objective. The desired dimensionality can be set
      using the parameter n_components.
* The goal of NCA is to learn an optimal transformation matrix L of size
  (`n_components, n_features`), which maximizes the sum over all samples i of
  probability `p_i` that i is classified correctly, i.e.,
  `arg max_{L} \sum_{i=1}^N p_i`
  where
  `p_i = \sum_{j \in C_i} p_{ij}`
  where `C_i` is the set of points in the same class as sample i and `p_{ij}` is the
  softmax over Euclidean distances  in the embedded space:
  `p_{ij} = \frac{\exp(- ||Lx_i - Lx_j||^2)}{sum_{k \neq i}exp(- ||Lx_i -
  Lx_k||^2)}, p_ii = 0`
    - Thus, NCA can be seen as learning a (squared) Mahalanobis distance metric:
      `|| L(x_i - x_j) ||^2 = (x_i - x_j)^T M (x_i - x_j)` where `M = L^T L` is
      a symmetric positive semi-definite matrix of size `(n_features, n_features)`

## Gaussian Processes

* Advantages:
    - prediction interpolates the observations
    - prediction is probabilistic (Gaussian) allowing computation of confidence
      intervals
    - very versatile, as can be used with many kernels
* Disadvantages:
    - not sparse, i.e., use all training data & features to perform predictions
    - inefficient in high-dimensional spaces
* Binning is a method to convert number into categorical
    - also how tree ensembles handle numeric features
    - allow for non-monotonic relationships
* `GaussianProcessRegressor` implements GP for regression purposes, where the
  prior mean is assumed to be constant & zero if `normalize_y` parameter is set
  to False or the training data's mean if set to True.
    - The prior's covariance is specified by passing a kernel object, whose
      hyperparameters are optimized during fitting by maximizing the log
      marginal likelihood (LML) based on the optimizer.
    - As LML may have multiple local minima, the optimizer can be started
      repeatedly by specifying `n_restarts_optimizer`.
* The noise level in the data can be specified explicitly by specifying the
  `alpha` parameter or including a WhiteKernel component into the kernel, which
  estimates the global noise level from data
* GaussianProcessRegressor allows prediction without fitting (based on the GP
  prior)
* Even though Kernel ridge regression & GPR both use the kernel trick internally
  to learn a target function, GPR uses gradients to determine the optimum values
  of the kernel hyperparameters, whereas the former must employ an expensive
  GridSearchCV for the same purpose and also, GPR is a generative model that can
  give confidence bounds for each prediction, whereas the former cannot.
* `GaussianProcessClassifier` implements GP for classification, i.e., test
  predicitons are class probabilities and supports multi-class classification
  either OvO or OvR.
    - OvO is computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that “one_vs_one” does not support predicting probability estimates but only plain predictions.
    - It places a GP prior on a latent function ("nuisance function") f, which
      is then squashed through a link function, viz., logistic link function,
      to obtain probabilistic classification
    - In contrast to the regression setting, the posterior of the latent function  is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation
* Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values  and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. 
* Kernels are parameterized by a vector \theta of hyperparameters, which can for
  instance control length-scales or periodicity of a kernel. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization.
* All Gaussian process kernels are interoperable with sklearn.metrics.pairwise and vice versa
* Kernel operators take one or two base kernels and combine them into a new
  kernel, e.g., Sum kernel takes two kernels and returns their sum, similarly
  for product and exponent kernels, the latter takes a base kernel and a scalar
  exponent parameter and returns the base^exponent.
* RBF is a stationary kernel parameterized by a length-scale parameter, which if
  l implies isotropic or if vector implies anisotropic
* The Matern kernel is a stationary kernel and a generalization of the RBF
  kernel. It has an additional parameter \nu  which controls the smoothness of the resulting function.
* The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales.
`k(x_i, x_j) = (1 + d(x_i, x_j)^2/2*\alpha*l^2) ^ -\alpha`
where `\alpha` is the scale mixture parameter
    - it currently only supports the isotropic variant, i.e., scalar l
* The ExpSineSquared kernel allows modeling periodic functions. It is
  parameterized by a length-scale parameter l and a periodicity parameter p
  `k(x_i, x_j) = exp(-2(sin(\pi/p*d(x_i,x_j))/l)^2)`
* The DotProductKernel is non-stationary and rotation invariant (but not
  translation-invariant) and is given by
`k(x_i, x_j) = \sigma_0^2 + x_i \cdot \x_j`
    - For `\sigma_0=0`, it's called "homogenous linear kernel"

## Cross-decomposition

* The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the canonical correlation analysis (CCA).
* These families of algorithms are useful to find linear relations between two multivariate datasets: the X and Y arguments of the fit method are 2D arrays.
* Cross decomposition algorithms find the fundamental relations between two matrices (X and Y). They are latent variable approaches to modeling the covariance structures in these two spaces. They will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS-regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases.


## Naive Bayes

* Naive Bayes are a set of supervised learning algorithms based on applying
  Bayes' theorem with the "naive" assumption of conditional independence among
  features for each class label, i.e.,
  P(y|x1, .., xn) = \frac{P(y)\prod_{i=1}^n P(x_i|y)}{P(x1,..., xn)}
  so the classification rule is based on:
  \hat{y} = arg max P(y) \prod_{i=1}^n P(x_i|y),
  where P(y) is the relative frequency of class y in the training set.
    - The different Bayes' classifiers are based on different assumptions about
      the form of P(x_i | y)
* Advantages of Naive Bayes' (NB) classifiers:
    - In spite of the strong assumption of conditional independence between
      features, they work very well in practice, most famously in spam email
      detection and document classification.
    - They're very fast since the assumption of independence allows them to
      avoid the curse of dimensionality
* On the flip side, even though they're good classifiers they're very poor at
  predicting class probabilities.

### Gaussian NB

* `GaussianNB` assumes
`P(x|i | y) = 1/\sqrt{2\pi\sigma_y^2} \exp{-(x_i - \mu_y)^2/(2\sigma_y^2)}`, where
the parameters `\mu_y, \sigma_y` are estimated by max. likelihood

### Multinomial NB

* It's one of 2 classic NB algorithms for text classification, where the data is
  coded as word counts
* It assumes
`P(x_i | y) = (N_{yi} + \alpha)/( N_y + \alpha*n)`,
where `N_{yi}` is the no. of times feature i appears in a sample of class y in the
training set, `N_y = \sum_i N_{yi}` and `\alpha` are smoothing priors to account for
features not occurring in the training set and prevents zero probabilities in
computations.
* `\alpha=1` is called Laplace smoothing
* `\alpha < 1` is called Lindstone smoothing

### Complement NB

* CNB is an adaptation of `MultinomialNB` particularly suited for unbalanced
  datasets, since it uses statistics from the complement of each class to
  compute the model weights and it's found to empirically outperform
  `MultinomialNB` on text classification
* Math:
```
P(x_i | y) = (\alpha_i + \sum_{j: z \neq y} d_{ij}) / (\alpha + \sum_{j: z
  \neq y} \sum_k d_{kj}),
  w_{yi} = log P(x_i | y),
  w_{yi} = w_{yi} / \sum_j w_{yj}
```
  where the summations are over all documents j not in class y, `d_{ij}` is either
  the count or tf-idf value of term i in document j, `\alpha_i` is a smoothing
  parameter, a la Multinomial NB, `\alpha = \sum_i \alpha_i`
    - The second normalization addresses the tendency for longer documents to
      dominate parameter estimates in MNB (multinomial NB)
* The classification rule is:
    `y = arg min_y \sum_i t_i w_{yi}`,
    i.e., a document is assigned to the class that is the poorest complement
    match.

### Bernoulli NB

* BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a BernoulliNB instance may binarize its input (depending on the binarize parameter).

* The decision rule is based on:
`P(x_i | y) = P(i | y ) x_i + (1 - P(i | y)) (1-x_i)`
    - Unlike MNB, it explicitly penalizes the non-occurrence of a feature i that
      is an indicator for class y, whereas in MNB, we simply ignore a
      non-occurring feature.
* In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. BernoulliNB might perform better on some datasets, especially those with shorter documents.

### Out-of-core learning

* All NB classifiers support out-of-core learning, i.e., when the training set
  is too large to fit in memory, by exposing a `partial_fit` method as well as
  sample weighting.
* Contrary to `fit`, `partial_fit` must be passed   the list of all expected
  class labels during the first call.


## Decision Trees

* Non-parametric classification and regression supervised learning method.
* Some salient advantages:
    - data requires little preparation, no need of normalization, dummy
      variables, etc.
    - inference cost logarithmic in training set size.
    - can handle numeric and categorical data as well as multi-output problems.
    - uses a white-box model, easily interpretable
    - can be validated using statistical tests
    - robust
* Some disadvantages:
    - prone to overfitting and can be very unstable -- can be mitigated by using
      ensembles
    - since the problem of finding the optimal decision tree is NP-complete, must
      use heuristic methods, e.g., greedy algorithms -- also mitigated by
      ensemble learning
    - cannot learn concepts like XOR, parity or multiplexing easily
    - can create biased learners if some classes dominate, so imp. to balance classes
      before fitting decision trees.

### Classification

* `DecisionTreeClassifier` is a class capable of multi-class classification.
    - It has a `predict_proba` method to give probabilities for different
      classes.
* `plot_tree` method can be used to plot the decision tree after fitting.
    - Alternatively, use `export_graphviz` method to export in GraphViz format.
    - `export_text` method from sklearn.tree.export exports the tree in textual format that can be
      printed on the screen.

### Regression

* Performed by the `DecisionTreeRegressor` class

### Complexity

* The run time cost of constructing a balanced binary tree is O(nplog(n)), where
  n is  no. of samples and p is no. of features, due to optimization performed
  in sklearn.
    - In general it would be O(n^2plog(n)), since (assuming the subtrees are
      approximately balanced), the cost at each node consists of searching
      through O(p) features to find the feature that offers the largest
      reduction in entropy, which is O(nplog(n)), so total cost would be as
      above (since for a fully developed tree, there are as many nodes as
      samples).

* Decision trees tend to overfit the most in high dimensional spaces with very few
  samples, so it's helpful to do dim. reduction such as PCA before fitting a
  decision tree.
* Use `max_depth=3` as an initial tree depth to visualize how the tree is
  fitting to the data, this parameter is extremely useful to control the
  overfitting.
* Use min_samples_split or min_samples_leaf to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try min_samples_leaf=5 as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While min_samples_split can create arbitrarily small leaves, min_samples_leaf guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, min_samples_leaf=1 is often the best choice.
* Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (sample_weight) for each class to the same value. Also note that weight-based pre-pruning criteria, such as min_weight_fraction_leaf, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like min_samples_leaf.
* If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as min_weight_fraction_leaf, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.
* All decision trees use np.float32 arrays internally. If training data is not in this format, a copy of the dataset will be made.
* If the input matrix X is very sparse, it is recommended to convert to sparse csc_matrix before calling fit and sparse csr_matrix before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.
* The classic ID3 (Iterative Dichotomizer 3) algo was developed in 1986 by Rob
  Quinlan.
    - The currently most popular CART (classification and regression trees) algo
      is very similar to the C4.5 algo, which was the successor to ID3.

### Mathematical formulation

* A decision tree recursively partitions the space such that the samples with
  the same labels are grouped together.
* If Q represents the data at a node m, for each candidate split `\theta = (j, t_m)`
  consisting of a feature j and threshold t_m, partition the data into left
  `Q_l` and `Q_r` subsets, where `Q_l = (x,y) | x_j <= t_m, Q_r = Q \ Q_l`, the
  impurity at m is computed using an impurity function H(), which depends on the
  task (classification/regression), as:
  `G(Q,\theta) = (n_l / N) *  H(Q_l) + (n_r / N) * H(Q_r)`
  The parameters are selected that minimize the impurity
  `\theta* = argmin_\theta G(Q,\theta)`
  Then we recurse for subsets `Q_l, Q_r`, until max depth is reached,
  `N_m < min_samples` or `N_m = 1`.
* If a target is a classification outcome taking  on values 0,...,K-1
  representing a region `R_m` with `N_m` observations, let:
  `p_{mk} = 1/N_m \sum_{x \in R_m} I(y_i = k)`
  be the proportion of class k observations in node m.
    - Common measures of impurity for training data `X_m` in node m are:
        * Gini: `H(X_m) = \sum_k p_{mk} (1-p_{mk})`
        * Entropy: `H(X_m) = - \sum_k p_{mk} log(p_{mk})`
        * Misclassification: `H(X_m) = 1 - max(p_{mk})`
* If a target is a continuous value, then for a node m, representing a region
  `R_m`  with `N_m` observations, common criteria to minimize as for determining the
  locations  for future splits are:
    - MSE: `\bar{y}_m = 1/N_m \sum_{i \in N_m} y_i, H(X_m) = 1/N_m \sum_{i \in N_m} (y_i - \bar{y}_m)^2`
    - MAE: `\bar{y}_m = 1/N_m \sum_{i \in N_m} y_i, H(X_m) = 1/N_m \sum_{i \in N_m} |y_i - \bar{y}_m|`

## Ensemble methods

* Goal is to combine predictions of several base estimators to decrease
  generalization error
* Two families of ensemble methods:
    - averaging methods: build several estimators independently and average
      their predictions
    - boosting methods: build base estimators sequentially to reduce the bias of
      the ensemble
* In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction.
* As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).
* 4 kinds of bagging:
    - pasting: drawing random subsets of data without replacement
    - bagging: drawing random subsets of data with replacement
    - random subspaces: drawing random subsets of features
    - random patches: drawing random subsets of features and samples
* In scikit-learn, bagging methods are offered as a unified BaggingClassifier
  meta-estimator (resp. BaggingRegressor), taking as input a user-specified base
  estimator along with parameters specifying the strategy to draw random
  subsets.
*  `max_samples` and `max_features` control the size of the subsets (in terms of
   samples and features), while `bootstrap` and `bootstrap_features` control
   whether samples and features are drawn with or without replacement. When
   using a subset of the available samples the generalization accuracy can be
   estimated with the out-of-bag samples by setting `oob_score=True`
* The sklearn.ensemble module includes two averaging algorithms based on randomized decision trees: the RandomForest algorithm and the Extra-Trees method.
* In random forests (see RandomForestClassifier and RandomForestRegressor classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features.
    - In contrast to the original publication, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.
    - In extremely randomized trees (see ExtraTreesClassifier and ExtraTreesRegressor classes),  as in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule.
    - The main parameters to adjust when using these methods is `n_estimators`
      and `max_features`. The former is the number of trees in the forest. The
      larger the better, but also the longer it will take to compute. In
      addition, note that results will stop getting significantly better beyond
      a critical number of trees. The latter is the size of the random subsets
      of features to consider when splitting a node. The lower the greater the
      reduction of variance, but also the greater the increase in bias.
      Empirical good default values are `max_features=n_features` for regression
      problems, and `max_features=sqrt(n_features)` for classification tasks.
* The relative rank (i.e. depth) of a feature used as a decision node in a tree
  can be used to assess the relative importance of that feature with respect to
  the predictability of the target variable since features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.  By averaging the estimates of predictive ability over several randomized trees one can reduce the variance of such an estimate and use it for feature selection. This is known as the mean decrease in impurity, or MDI.
    - those estimates are stored as an attribute named feature_importances_ on the fitted model. This is an array with shape (n_features,) whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.
* RandomTreesEmbedding implements an unsupervised transformation of the data. Using a forest of completely random trees, RandomTreesEmbedding encodes the data by the indices of the leaves a data point ends up in. This index is then encoded in a one-of-K manner, leading to a high dimensional, sparse binary coding. This coding can be computed very efficiently and can then be used as a basis for other learning tasks. The size and sparsity of the code can be influenced by choosing the number of trees and the maximum depth per tree. For each tree in the ensemble, the coding contains one entry of one. The size of the coding is at most n_estimators * 2 ** max_depth, the maximum number of leaves in the forest.  As neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit, non-parametric density estimation.
* The module sklearn.ensemble includes the popular boosting algorithm AdaBoost,
  introduced in 1995 by Freund and Schapire.
    - The core principle of AdaBoost is to fit a sequence of weak learners
      (i.e., models that are only slightly better than random guessing, such as
      small decision trees) on repeatedly modified versions of the data. The
      predictions from all of them are then combined through a weighted majority
      vote (or sum) to produce the final prediction. The data modifications at
      each so-called boosting iteration consist of applying weights w1,... wn to
      each of the training samples. Initially, those weights are all set to 1/N, so
      that the first step simply trains a weak learner on the original data. For
      each successive iteration, the sample weights are individually modified
      and the learning algorithm is reapplied to the reweighted data. At a given
      step, those training examples that were incorrectly predicted by the
      boosted model induced at the previous step have their weights increased,
      whereas the weights are decreased for those that were predicted correctly.
      As iterations proceed, examples that are difficult to predict receive
      ever-increasing influence. Each subsequent weak learner is thereby forced
      to concentrate on the examples that are missed by the previous ones in the
      sequence.
    - The number of weak learners is controlled by the parameter n_estimators. The learning_rate parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps, i.e., a decision tree of depth 1. Different weak learners can be specified through the base_estimator parameter. The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples to consider a split min_samples_split).
* Gradient Tree Boosting or Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient Tree Boosting models are used in a variety of areas including Web search ranking and ecology.
    - Advantages: can handle different datatypes easily, good predictive power
      and robustness to outliers
    - Disadvantages: scalability due to sequential nature of boosting.
    - `GradientBoostingClassifier` supports both binary and multi-class classification
    - `GradientBoostingRegressor` supports a number of different loss functions for regression which can be specified via the argument loss; the default loss function for regression is least squares ('ls').
    - Both GradientBoostingRegressor and GradientBoostingClassifier support warm_start=True which allows you to add more estimators to an already fitted model.
    - The size of the regression tree base learners defines the level of variable
  interactions that can be captured by the gradient boosting model. In general,
  a tree of depth h can capture interactions of order h.
        * We found that max_leaf_nodes=k gives comparable results to max_depth=k-1 but is significantly faster to train at the expense of a slightly higher training error.
    - GBRT considers additive models of the following form
      `F(x) = \sum_{m=1}^M \gamma_m h_m(x)`, where `h_m(x)` are the base learners/"weak learners"
    - Similar to other boosting algorithms, GBRT builds the additive model in a
      greedy fashion
      `F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)`,
      where the newly added tree `h_m` tries to minimize the loss L, given the
      ensemble `F_{m-1}`:
      `h_m = argmin_h \sum_{i=1}^m L(y, F_{m-1}(x) + h(x_i))`
        * The initial model is problem specific, for least-squares regression
          one usually chooses the mean of the target values, but can also be
          specified via the `init` argument.
        * Gradient Boosting attempts to solve this minimization problem
          numerically via steepest descent: The steepest descent direction is
          the negative gradient of the loss function evaluated at the current
          model  which can be calculated for any differentiable loss function,
          where the step length `\gamma_m` is chosen by line search.
        * Loss functions for regression include: least squares, least absolute
          deviaton, huber, quantile
        * Loss functions for classification include: binomimal or multinomial
          deviance or exponential
    - A simple regularization strategy is to scale the contribution of each weak
      learner by a factor `\nu`: `F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x)`, where
      `\nu` is the learning rate (as it scales the step  length of the gradient
      descent)
        * The parameter learning_rate strongly interacts with the parameter n_estimators, the number of weak learners to fit. Smaller values of learning_rate require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of learning_rate favor better test error. 
        * stochastic gradient boosting combines gradient boosting with bootstrap averaging (bagging). At each iteration the base classifier is trained on a fraction subsample of the available training data. The subsample is drawn without replacement. A typical value of subsample is 0.5.
        * The feature importance scores of a fit gradient boosting model can be accessed via the `feature_importances_` property
    * The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.

## Multiclass and multilabel algorithms
* The `sklearn.multiclass` module implements meta-estimators to solve multiclass
  and multilable classification problems by decomposing such problems into
  binary classification problems.
    - Multitarget regression is also supported
    - All classifiers in sklearn support multiclass classification out of the
      box, so the classes in this module are useful only for experimenting with
      different strategies for performing multiclass classification.
* Multiclass classification => classification task with more than 2 mutually
  exclusive labels, e.g., iris classification task
* Multilabel classification => classification task with multiple labels that may
  not be mutually exclusive, e.g., document classification into topics
* Multioutput regression => predicting several numerical quantities, e.g., wind
  speed and direction at each point in a field
* Multioutput-multiclass and multi-task classification => single estimator to
  handle several joint classification tasks, a generalization of multilabel and
  multiclass.
* The MultiLabelBinarizer transformer can be used to convert between a collection of collections of labels and the indicator format.
```
>>>
>>> from sklearn.preprocessing import MultiLabelBinarizer
>>> y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]
>>> MultiLabelBinarizer().fit_transform(y)
array([[0, 0, 1, 1, 1],
       [0, 0, 1, 0, 0],
       [1, 1, 0, 1, 0],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 0, 0]])
```
* `OneVsOneClassifier` may be advantageous over `OneVsRestClassifier` for
  algorithms such as kernel algorithms which don't scale well with `n_samples`,
  as each classifier has to deal with only a subset of the training set, which
  can offset its `O(n_classes^2)` complexity.

### Error-Correcting Output-Codes
* Output-code based strategies are fairly different from one-vs-the-rest and one-vs-one. With these strategies, each class is represented in a Euclidean space, where each dimension can only be 0 or 1. Another way to put it is that each class is represented by a binary code (an array of 0 and 1). The matrix which keeps track of the location/code of each class is called the code book. The code size is the dimensionality of the aforementioned space. Intuitively, each class should be represented by a code as unique as possible and a good code book should be designed to optimize classification accuracy. In this implementation, we simply use a randomly-generated code book as advocated in  although more elaborate methods may be added in the future.
At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen.
In OutputCodeClassifier, the `code_size` attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.
A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, `log2(n_classes) / n_classes` is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since `log2(n_classes)` is much smaller than `n_classes`.
A number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name "error-correcting:. In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging.

* Multioutput regression support can be added to any regressor with MultiOutputRegressor. This strategy consists of fitting one regressor per target, and so it doesn't take advantage of correlations between targets.

* Multioutput classification support can be added to any classifier with MultiOutputClassifier. This strategy consists of fitting one classifier per target.

* Classifier chains (see ClassifierChain) are a way of combining a number of binary classifiers into a single multi-label model that is capable of exploiting correlations among targets.  
For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number.  
When predicting, the true labels will not be available. Instead the predictions of each model are passed on to the subsequent models in the chain to be used as features.  
Clearly the order of the chain is important. The first model in the chain has no information about the other labels while the last model in the chain has features indicating the presence of all of the other labels. In general one does not know the optimal ordering of the models in the chain so typically many randomly ordered chains are fit and their predictions are averaged together.
* Regressor chains (see RegressorChain) is analogous to ClassifierChain as a way of combining a number of regressions into a single multi-target model that is capable of exploiting correlations among targets.

## Feature selection
* VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.
* Univariate feature selection works by selecting the best features based on univariate statistical tests.
Scikit-learn exposes the foll. feature selection routines as objects that implement the transform method:
    - SelectKBest removes all but the  highest scoring features [here score can
      be any statistical test that gives a score, e.g, chi2]
    - SelectPercentile removes all but a user-specified highest scoring percentage of features
using common univariate statistical tests for each feature: false positive rate SelectFpr, false discovery rate SelectFdr, or family wise error SelectFwe.
    - GenericUnivariateSelect allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.
    - These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile):
        * For regression: `f_regression, mutual_info_regression`
        * For classification: `chi2, f_classif, mutual_info_classif`
            - The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.
    - Beware not to use a regression scoring function with a classification problem, you will get useless results.
* Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (`RFE`) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a `coef_` attribute or through a `feature_importances_` attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.
    - `RFECV` performs RFE in a cross-validation loop to find the optimal number of features.
* SelectFromModel is a meta-transformer that can be used along with any estimator that has a `coef_` or `feature_importances_` attribute after fitting. The features are considered unimportant and removed, if the corresponding `coef_` or `feature_importances_` values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like `“0.1*mean”`.
* For a good choice of alpha, the Lasso can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be "sufficiently large", or L1 models will perform at random, where "sufficiently large" depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated.
    - There is no general rule to select an alpha parameter for recovery of non-zero coefficients. It can by set by cross-validation (LassoCV or LassoLarsCV), though this may lead to under-penalized models: including a small number of non-relevant variables is not detrimental to prediction score. BIC (LassoLarsIC) tends, on the opposite, to set high values of alpha.
* Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a sklearn.pipeline.Pipeline:
```
clf = Pipeline([
  ('feature_selection', SelectFromModel(LinearSVC(penalty="l1"))),
  ('classification', RandomForestClassifier())
])
clf.fit(X, y)
```

## Semi-supervised
* The algorithms in `sklearn.semi_supervised` can perform well when we have a very small amount of labeled points and a large amount of unlabeled points.
* scikit-learn provides two label propagation models: LabelPropagation and LabelSpreading. Both work by constructing a similarity graph over all items in the input dataset.
* LabelPropagation and LabelSpreading differ in modifications to the similarity matrix that graph and the clamping effect on the label distributions. Clamping allows the algorithm to change the weight of the true ground labeled data to some degree. The LabelPropagation algorithm performs hard clamping of input labels, which means `\alpha = 0`. This clamping factor can be relaxed, to say `\alpha = 0.2`, which means that we will always retain 80 percent of our original label distribution, but the algorithm gets to change its confidence of the distribution within 20 percent.
LabelPropagation uses the raw similarity matrix constructed from the data with no modifications. In contrast, LabelSpreading minimizes a loss function that has regularization properties, as such it is often more robust to noise. The algorithm iterates on a modified version of the original graph and normalizes the edge weights by computing the normalized graph Laplacian matrix. This procedure is also used in Spectral clustering.
Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available:
rbf (`exp(-\gamma |x-y|^2), \gamma > 0`).`\gamma`  is specified by keyword gamma.
knn (`1[x' \in kNN(x)]`). `k` is specified by keyword `n_neighbors`.
The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.

## Isotonic regression
* The class IsotonicRegression fits a non-decreasing function to data by solving
  the problem:
  ```
  min \sum_i w_i (y_i - \hat{y}_i)^2
  s.t. \hat{y}_{min} = \hat{y}_1 \leq \hat{y}_2 \ldots \leq \hat{y}_n =
  \hat{y}_{max}
  ```
  where each `w_i` is a strictly positive number and each `y_i` is an arbitrary
  real number.
*  It yields the vector which is composed of non-decreasing elements the closest in terms of mean squared error. In practice this list of elements forms a function that is piecewise linear.

## Probability calibration
* Well calibrated classifiers are probabilistic classifiers for which the output of the `predict_proba` method can be directly interpreted as a confidence level, e.g., for a well-calibrated binary classifier, for all samples with `predict_proba` near 0.8, approximately 80% of them should actually belong to the positive class.
* LogisticRegression returns well calibrated predictions by default as it directly optimizes log-loss.
* GaussianNB tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features.
* RandomForestClassifier shows the opposite behavior: the histograms show peaks at approximately 0.2 and 0.9 probability, while probabilities close to 0 or 1 are very rare. This is because methods such as bagging and random forests have 1-sided errors near 0 and 1 caused by some of the trees.
* Linear Support Vector Classification (LinearSVC) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods, which focus on hard samples that are close to the decision boundary (the support vectors).
* Two approaches for performing calibration of probabilistic predictions are provided:
    - a parametric approach based on Platt’s sigmoid model and
    - a non-parametric approach based on isotonic regression (sklearn.isotonic).
* Probability calibration should be done on new data not used for model fitting.
* The class CalibratedClassifierCV uses a cross-validation generator and estimates for each split the model parameter on the train samples and the calibration of the test samples. The probabilities predicted for the folds are then averaged.
    - Already fitted classifiers can be calibrated by CalibratedClassifierCV via the parameter cv=”prefit”. In this case, the user has to take care manually that data for model fitting and calibration are disjoint.

## Neural network models (supervised)
* sklearn's implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support.
* The module contains the public attributes `coefs_` and `intercepts_`.
    - `coefs_` is a list of weight matrices, where weight matrix at index i represents the weights between layer i and layer i+1.
    - `intercepts_` is a list of bias vectors, where the vector at index i represents the bias values added to layer i+1.
* MLP is sensitive to feature scaling.
* Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation.
    - Currently, MLPClassifier supports only the Cross-Entropy loss function, which allows probability estimates by running the `predict_proba` method.
    - MLPClassifier supports multi-class classification by applying Softmax as the output function
    - Further, the model supports multi-label classification in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to 0.5 are rounded to 1, otherwise to 0. For a predicted output of a sample, the indices where the value is 1 represents the assigned classes of that sample
* Class MLPRegressor implements a multi-layer perceptron (MLP) that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function. Therefore, it uses the square error as the loss function, and the output is a set of continuous values.
    - MLPRegressor also supports multi-output regression, in which a sample can have more than one target.
* Both MLPRegressor and MLPClassifier use parameter alpha for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes.
* MLP trains using Stochastic Gradient Descent, Adam, or L-BFGS.
    - Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update parameters based on adaptive estimates of lower-order moments.
    - With SGD or Adam, training supports online and mini-batch learning, but
      not with L-BFGS
* For n training samples with m features, a NN with k hidden layers each
  containing h neurons and o output neurons has time complexity for
  backpropagation `O(n*m*h^k*o*i)`, where i is the no. of iterations.
    - Since backpropagation has a high time complexity, it is advisable to start with smaller number of hidden neurons and few hidden layers for training.
* Finding a reasonable regularization parameter  is best done using GridSearchCV, usually in the range `10.0 ** -np.arange(1, 7)`.
* Empirically, we observed that L-BFGS converges faster and with better solutions on small datasets. For relatively large datasets, however, Adam is very robust. It usually converges quickly and gives pretty good performance. Stochastic Gradient Descent with momentum or nesterov’s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned.
* If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using `warm_start=True` and `max_iter=1` and iterating yourself can be helpful

# Unsupervised learning

## Gaussian mixture models

* Found in package `sklearn.mixture`
* Support diagonal, spherical, tied and full covariance matrices, which
  determine the directions and lengths of the axes of its density contours, all
  of which are ellipsoids.
    - Full means the components may independently adopt any position and shape.
    - Tied means they have the same shape, but the shape may be anything.
    - Diagonal means the contour axes are oriented along the coordinate axes, but otherwise the eccentricities may vary between components.
    - Tied Diagonal is a "tied" situation where the contour axes are oriented along the coordinate axes.
    - Spherical is a "diagonal" situation with circular (in 2-D) contours (spherical in dimensions higher than 2, whence the name).
* A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.
    - One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.
* The `GaussianMixture` object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models.
    - It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data (In theory, it recovers the true number of components only in the asymptotic regime, i.e., if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution).
    - It is the fastest algorithm for learning mixture models.
        * Expectation-maximization is a well-founded statistical algorithm
          consisting of two steps repeated iteratively until convergence into a
          local maximum is achieved:
            i. assume random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model,
            i. tweak the parameters to maximize the likelihood of the data given those assignments.
    - As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply.
    - When one has insufficiently many points per mixture, estimating the covariance matrices becomes difficult, and the algorithm is known to diverge and find solutions with infinite likelihood unless one regularizes the covariances artificially.
    - This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues.
* The `BayesianGaussianMixture` object implements a variant of the Gaussian mixture model with variational inference algorithms.
    - Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood.
    - The principle behind variational methods is the same as expectation-maximization (that is both are iterative algorithms that alternate between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points), but variational methods add regularization by integrating information from prior distributions. This avoids the singularities often found in expectation-maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not usually as much so as to render usage unpractical.
    - Due to its Bayesian nature, the variational algorithm needs more hyper-parameters than expectation-maximization, the most important of these being the concentration parameter `weight_concentration_prior`. Specifying a low value for the concentration prior will make the model put most of the weight on few components set the remaining components weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture.
    - Variational Bayes can be seen as an extension of the EM (expectation-maximization) algorithm from maximum a posteriori estimation (MAP estimation) of the single most probable value of each parameter to fully Bayesian estimation which computes (an approximation to) the entire posterior distribution of the parameters and latent variables.
    - The parameters implementation of the BayesianGaussianMixture class proposes two types of prior for the weights distribution:
        i. a finite mixture model with Dirichlet distribution and,
        i. an infinite mixture model with the Dirichlet Process.
            * Dirichlet process is an infinite-dimensional version version of Dirichlet
  distribution, a family of continuous multivariate probability distributions,
  a multivariate generalization of the beta distribution.
    - when `weight_concentration_prior` is small enough and `n_components` is larger than what is found necessary by the model, the Variational Bayesian mixture model has a natural tendency to set some mixture weights values close to zero. This makes it possible to let the model choose a suitable number of effective components automatically. Only an upper bound of this number needs to be provided. Note however that the “ideal” number of active components is very application specific and is typically ill-defined in a data exploration setting.
    - unlike finite models, which will almost always use all components as much as they can, and hence will produce wildly different solutions for different numbers of components, the variational inference with a Dirichlet process prior (`weight_concentration_prior_type='dirichlet_process'`) won’t change much with changes to the parameters, leading to more stability and less tuning.
    - due to the incorporation of prior information, variational solutions have less pathological special cases than expectation-maximization solutions.
    - this algorithm needs an extra hyperparameter that might need experimental tuning via cross-validation.
    - there are many implicit biases in the inference algorithms (and also in the Dirichlet process if used), and whenever there is a mismatch between these biases and the data it might be possible to fit better models using a finite mixture.

## Manifold learning

* Manifold learning is an approach to non-linear dimensionality reduction.
    - Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.
    - Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data.
* One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Isomap can be performed with the object `Isomap`.
* Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.  Locally linear embedding can be performed with function `locally_linear_embedding` or its object-oriented counterpart `LocallyLinearEmbedding`.
    - One well-known issue with LLE is the regularization problem. When the number of neighbors is greater than the number of input dimensions, the matrix defining each local neighborhood is rank-deficient. To address this, standard LLE applies an arbitrary regularization parameter r, which is chosen relative to the trace of the local weight matrix. Though it can be shown formally that as r goes to 0, the solution converges to the desired embedding, there is no guarantee that the optimal solution will be found for r > 0. This problem manifests itself in embeddings which distort the underlying geometry of the manifold.
    - One method to address the regularization problem is to use multiple weight vectors in each neighborhood. This is the essence of modified locally linear embedding (MLLE). MLLE can be performed with function `locally_linear_embedding` or its object-oriented counterpart `LocallyLinearEmbedding`, with the keyword method = 'modified'. It requires `n_neighbors > n_components`.
    - Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, sklearn implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function `locally_linear_embedding` or its object-oriented counterpart LocallyLinearEmbedding, with the keyword `method = 'hessian'`. It requires `n_neighbors > n_components * (n_components + 3) / 2`.
* Spectral Embedding is implemented in sklearn as Laplacian Eigenmaps, which finds a low dimensional representation of the data using a spectral decomposition of the graph Laplacian, which can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space.
    - Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances.
    - Spectral embedding can be performed with the function `spectral_embedding` or its object-oriented counterpart `SpectralEmbedding`.
* Local Tangent Space Alignment (LTSA) seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function `locally_linear_embedding` or its object-oriented counterpart `LocallyLinearEmbedding`, with the keyword `method = 'ltsa'`.
* Multidimensional scaling (implemented by class `MDS`) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.
    - MDS attempts to model similarity or dissimilarity data as distances in a geometric spaces.
    - There exists two types of MDS algorithm:
        1. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data.
        1. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.

### t-SNE

* t-SNE (implemented by `TSNE`) converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Student’s t-distributions. This allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques:
    - Revealing the structure at many scales on a single map
    - Revealing data that lie in multiple, different, manifolds or clusters
    - Reducing the tendency to crowd points together at the center
* While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset.
* The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.
* The disadvantages to using t-SNE are roughly:
    - t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes
    - The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.
    - The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.
    - Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using init='pca').
* There are five parameters that control the optimization of t-SNE and therefore possibly the quality of the resulting embedding:
    - perplexity
    - early exaggeration factor
    - learning rate
    - maximum number of iterations
    - angle (not used in the exact method)
* The perplexity is defined as `k=2^(S)` where S is the Shannon entropy of the conditional probability distribution. The perplexity of a k-sided die is k, so that k is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.
* The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten’s FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results.
* The Barnes-Hut improves the runtime complexity of the t-SNE by approximations
  from O(dN^2) to O(dNlogN), parameterized by the `angle` parameter.

### General tips
* Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly if different scales are used with different features.
* The reconstruction error computed by each routine can be used to choose the
  optimal output dimension, which will decrease monotonically with increasing
  `n_components`
* Noisy data can “short-circuit” the manifold, in essence acting as a bridge between parts of the manifold that would otherwise be well-separated.
* Totally Random Trees Embedding can also be useful to derive non-linear representations of feature space, also it does not perform dimensionality reduction.

## Clustering

* Clustering of unlabeled data can be performed with the module
  `sklearn.cluster`
* Each clustering algorithm comes in 2 variants:
    i. a class that implements the `fit` method on training data
        - The labels over the training data can be found in the `labels_`
          attribute.
    i. a function that takes training data as input and returns an array of
    integer labels corresponding to different clusters.
* See table of overview & comparison of different clustering methods at:
https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods
* Non-flat geometry clustering is useful when the clusters have a specific
shape, i.e. a non-flat manifold, and the standard euclidean distance is not the
right metric.

### K-Means

* KMeans can be seen as a special case of Gaussian mixture model with equal
  covariance per component.
* The KMeans algorithm clusters data by trying to separate samples in n groups
  of equal variance, minimizing a criterion known as the inertia or
  within-cluster sum-of-squares, i.e., the sum of squared distances of each
  point in a cluster to that cluster's centroid and as a consequence, suffers
  from the following 2 drawbacks:
    - doesn't perform well for elongated clusters or manifolds with irregular
      shapes, as inertia assumes clusters are convex & isotropic.
    - due to the curse of dimensionality, Euclidean distance becomes inflated
      in high-dimensions, making inertia a non-normalized metric.
* It scales well to large number of samples and has been used across a large range of application areas in many different fields.
* K-Means is often referred to as Lloyd's algorithm and has 3 steps:
    1. Choose initial centroids, e.g., randomly choosing k samples from train
    data and then loop through the following 2 steps until the centroids don't
    move significantly.
    1. Assign each sample to its nearest centroid
    1. Create new centroids by taking the mean value of all samples assigned to
    each centroid.
* K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.
* Given enough time, K-means will always converge, however this may be to a local minimum.
    - This is highly dependent on the initialization of the centroids.
    * One method to help address this issue is the k-means++ initialization
      scheme, which has been implemented in scikit-learn (use the
      `init='k-means++'` parameter), which initializes the centroids to be
      (generally) distant from each other, leading to provably better results
      than random initialization.

#### Mini-batch K-Means

* A variation of the vanilla K-Means that uses mini-batches to reduce
  computation time with only a slight decrease in quality of results.
* After drawing b samples from training data to form a mini-batch, they are
  assigned to tne nearest centroid and then the centroids are updated as in
  k-means, but on a per-sample basis, i.e., For each sample in the mini-batch,
  the assigned centroid is updated by taking the streaming average of the sample
  and all previous samples assigned to that centroid. This has the effect of
  decreasing the rate of change for a centroid over time.

### Affinity propagation

* Reference: https://pdfs.semanticscholar.org/ea78/2c8b0848987e9575ea648e0419054d3f5bbf.pdf
* Creates clusters by sending 2 kinds of messages (real-valued numbers) between pairs of samples until
  convergence:
    1. Responsibilities r(i,k): accumalated evidence that sample k could be
    exemplar for sample i, considering other exemplars
    1. Availabilities a(i,k): accumalated evidence that sample i should choose
    sample k to be its exemplar, considering all other samples that would use
    sample k as exemplar
    - As a result, exemplars are chosen by samples if they're similar enough to
      many samples & chosen by many samples to be representative of themselves.
* It automatically chooses the number of clusters based on data and is
  influenced by 2 parameters:
    - preference: controls how many exemplars are used
    - damping factor: avoids numerical oscillations in the responsibility and
      availability values during updates
* Runtime complexity `O(NT^2)`, where N is no. of samples & T is no. of
  iterations and memory requirements `O(N^2)`, making it unsuitable for large
  datasets

### Mean shift

* MeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.
* The algorithm is not highly scalable, as it requires multiple nearest neighbor
  searches during the execution of the algorithm.

### Hierarchical clustering

* Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). 
* The `AgglomerativeClustering` object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together.
* 4 different linkage criteria are available for determining which clusters to
  merge:
    1. Ward: minimizes the sum of squared differences within all clusters.
    1. Maximum/complete: minimizes the max. distance b/w observations of pairs
    of clusters
    1. Average: minimizes the average distances b/w all observations of pairs of
    clusters
    1. Single: minimizes the distance b/w the closest observations of pairs of
    clusters
* `AgglomerativeClustering` can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges.
* Agglomerative cluster has a “rich get richer” behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes
* connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll.
    - Connectivity constraints and single, complete or average linkage can enhance the ‘rich getting richer’ aspect of agglomerative clustering.
* Ward linkage supports only Euclidean (l2) distances, for l1, cosine or any
  other precomputed "affinity matrix" (matrix of distances between samples),
  must use one of the other 3 linkages.

### DBSCAN

* Views clusters as areas of high density separated by areas of low density.
    - This generic view allows it to find clusters of any shape, not necessarily
      convex.
* Uses the concept of "core samples", which are samples in areas of
  high-density.
* Defines a cluster as a set of core samples, each close to one another and a
  set of non-core samples that are close to a core sample.
* 2 main hyper-parameters:
    - `min_samples`: must have this many neighbors in order to be considered a
      core sample
    - `eps`: distance from a sample that defines its neighborhood
* Any sample that is not a core sample, and is at least eps in distance from any core sample, is considered an outlier by the algorithm.
* While the parameter min_samples primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desiable to increase this parameter), the parameter eps is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as -1 for “noise”). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster.
* The cluster assignment by sklearn's implementation of DBSCAN depends on the
  ordering of the data.

### OPTICS

* A generelization of DBSCAN that allows `eps` to take a range of values instead
  of a single value
* The key difference between DBSCAN and OPTICS is that the OPTICS algorithm
  builds a reachability graph, which assigns each sample both a `reachability_`
  distance, and a spot within the cluster `ordering_` attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership.

### BIRCH

* The Birch builds a tree called the Characteristic Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.
* The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:
    - Number of samples in a subcluster.
    - Linear Sum - A n-dimensional vector holding the sum of all samples
    - Squared Sum - Sum of the squared L2 norm of all samples.
    - Centroids - To avoid recalculation linear sum / n_samples.
    - Squared norm of the centroids.
* The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.
* This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by n_clusters. If n_clusters is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.
* Birch does not scale very well to high dimensional data. As a rule of thumb if n_features is greater than twenty, it is generally better to use MiniBatchKMeans.
* If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans.

### Clustering performace evaluation

#### Adjusted rand score

* a function that measures the similarity of the two assignments, ignoring
  permutations and with chance normalization as follows:
    - If C is the ground truth class assignment & K the clustering, let a be the
      no. of pairs of elements that are in the same set in C and in the same set
      in K & b be the no. of pairs of elements that are in different sets in C
      and in different sets in K. Then define the raw (unadjusted) Rand index
      as: (a+b)/nC2, where nC2 is the total no. of distinct pairs of samples. To
      guarantee that random label assignments get a value close to 0, define
      adjusted rand index (ARI) as: (RI - E[RI])/(max(RI) - E[RI]), where E[RI}
      is the expected rand index of random labelings
* Since it's symmetrics, it can be used as a consensus score.
* Range is [-1,1], with random labeling getting a value close to 0 and perfect
  match a score of 1.
* Another advantage of ARI is that no assumption is made on the cluster
  structure
* The main drawback of ARI is that it requires ground truth labels.

#### Mutual information based scores

* Mutual Information is a function that measures the agreement of the two assignments, ignoring permutations.
* All, `mutual_info_score`, `adjusted_mutual_info_score` and
  `normalized_mutual_info_score` are symmetric: swapping the argument does not change the score. Thus they can be used as a consensus measure
*  Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, an AMI of exactly 1 indicates that the two label assignments are equal (with or without permutation).
* The main drawback is that it requires knowledge of ground truth
* For 2 assignments U & V of the same object, define entropy as:
```
H(U) = -\sum_{i=1}^{|U|} P(i) log(P(i))
```
where `P(i) = |U_i|/N` is the probability that an object picked at random from U
falls into class U_i.
The mutual information (MI) between U & V is computed as:
```
MI(U,V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} P(i,j) log(P(i,j)/ (P(i) P(j)) )
```
where `P(i,j) = |U_i & V_j|/N` is the probability that a sample picked at random
is both in Ui and Vj.
Normalized mutual information (NMI) is defined as
```
NMI(U,V)=MI(U,V)/mean(H(U),H(V))
```
This value of the mutual information and also the normalized variant is not
adjusted for chance and will tend to increase as the number of different labels
(clusters) increases, regardless of the actual amount of “mutual information”
between the label assignments. Using the expected value  for the mutual
information (obtained from a complicated formula), the adjusted mutual information can then be calculated using a similar form to that of the adjusted Rand index:
AMI = (MI - E[MI])/(mean(H(U), H(V)) - E[MI])
THe mean can be arithmetic or geometric depending on application

#### Homogeneity, completeness and V-measure

* Homogenity score, which is a measure of how much each cluster contains only
  members of a single class, is defined as:
```
h = 1 - H(C|K)/H(C)
```
where `H(C|K)` is the conditional entropy of classes given the cluster
assignments defined as:
```
H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} n_{c,k}/n log(n_{c,k}/n)
```
where n is the total number of samples & `n_{c,k}` is the number of samples of
class c assigned to cluster k and `H(C)` is the entropy of classes defined by:
```
H(C) = - \sum_{c=1}^{|C|} n_c/n log(n_c/n)
```
where `n_c` is the number of samples belonging to class c.
* Completeness score, which is a measure of how much all members of a given
  class are assigned to the same cluster, is defined as:
  ```
  c = 1-H(K|C)/H(K)
  ```
  where H(K|C) and H(K) are the conditional entropy of clusters given classes
  and the entropy of clusters, defined symmetrically as before.
* V-measure is the (weighted) harmonic mean of the homogenity and completeness
  score, defined as:
  ```
  v = \frac{(1 + \beta) \times \text{homogeneity} \times \text{completeness}}{(\beta \times \text{homogeneity} + \text{completeness})}
  ```
    - beta < 1.0 will attribute more weight to homogenity and vice versa.
* Advantages:
    - Bounded between 0 & 1
    - No assumption on cluster structure
* Disadvantages:
    - Need ground truth labels
    - not normalized with regards to random labeling: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular random labeling won’t yield zero scores especially when the number of clusters is large.
        * This problem can safely be ignored when the number of samples is more than a thousand and the number of clusters is less than 10. For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI).

#### Fowlkes-Mallows scores

* Geometric mean of pairwise precision and recall:
```
FMI = TP / sqrt{(TP+FP) (TP+FN)}
```
* TP => no. of pairs that belong to the same clusters in both true &
  predicted labels
* FP => no. of pairs that belong to the same clusters in the true, but not the
  predicted labels
* FN => no. of pairs that belong to the same clusters in the predicted, but not
  the true labels.
* Advantages:
    - Bounded between 0 & 1, with 0 indicating independent labeling and 1
      indicating perfect labeling
    - no assumption on cluster structure
* Drawback is needs ground truth labels

### Silhoutte coefficient

* Defined for each sample as:
```
s = (b-a)/max(a,b)
```
where a is mean distance between a sample and all other points  in the same
cluster and b is the mean distance between a sample and all other points in the
next nearest cluster
* For a set of samples, the Sillhoute coefficient is taken as the mean of the
  Sillhoute coefficients for all the samples.
* Advantages:
    - doesn't need ground truth labels
    - bounded between -1 (incorrect clustering) to +1 (highly dense clustering),
      with values around 0 indicating overlapping clusters
    - score is higher when clusters are dense and well separated, corresponding
      to the intuition of a cluster
* Disadvantages:
    - higher for convex clusters than those for other shaped clusters, such as
      those found by DBSCAN

#### Calinski-Harabasz Index/Variance Ratio Criterion

* For k clusters, the score s is given by the ratio of the between-clusters
  dispersion mean and within-cluster dispersion (see
  https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index
  for exact formula)
* Doesn't need ground truth labels, higher when clusters are well separated and
  fast to compute, but also gives higher scores for convex clusters over others.


#### Davies-Bouldin Index

* defined as the average similarity between each cluster and its most similar
  one:
  ```
  DB = 1/k \sum_{i=1}^k max_{i \neq j} R_{ij}
  ```
  where k is no. of clusters and `R_{ij} = (s_i + s_j)/d_{ij}` is a similarity
  measure that trades off the cluster diameter `s_i`, i.e., avg. distance b/w
  each point of the cluster and its centroid and the distance between cluster
  centroids `d_{ij}`
* Doesn't need ground truth labels
* Zero is the lowest possible scores, and values closer to 0 indicate better
  partition
* Again higher for convex clusters
* Usage of centroid distances limits to Euclidean distances
* Lastly, a good value reported by this method does not imply the best information retrieval.


#### Contingency matrix

* reports the intersection cardinality for every true/predicted cluster pair.
* A confusion matrix for classification is a square contingency matrix where the order of rows and columns correspond to a list of classes.
* Contingency matrix is easy to interpret for a small number of clusters, but becomes very hard to interpret for a large number of clusters.


## Biclustering

* Biclustering algorithms simultaneously cluster rows and columns of a data matrix.
* Biclustering has many other names in different fields including co-clustering, two-mode clustering, two-way clustering, block clustering, coupled two-way clustering, etc. 
* If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix reveals the biclusters on the diagonal.
* In the checkerboard case, each row belongs to all column clusters, and each column belongs to all row clusters.

### Spectral co-clustering

* The SpectralCoclustering algorithm finds biclusters with values higher than
  those in the corresponding other rows and columns. Each row and each column
  belongs to exactly one bicluster, so rearranging the rows and columns to make
  partitions contiguous reveals these high values along the diagonal.
* The algorithm treats the input data matrix as a bipartite graph: the rows and
  columns of the matrix correspond to the two sets of vertices, and each entry
  corresponds to an edge between a row and a column. The algorithm approximates
  the normalized cut of this graph to find heavy subgraphs.

### Spectral biclustering

* The SpectralBiclustering algorithm assumes that the input data matrix has a
  hidden checkerboard structure. The rows and columns of a matrix with this
  structure may be partitioned so that the entries of any bicluster in the
  Cartesian product of row clusters and column clusters are approximately
  constant. For instance, if there are two row partitions and three column
  partitions, each row will belong to three biclusters, and each column will
  belong to two biclusters. The algorithm partitions the rows and columns of a
  matrix so that a corresponding blockwise-constant checkerboard matrix provides
  a good approximation to the original matrix.

### Biclustering evaluation

* There are two ways of evaluating a biclustering result: internal and external.
  Internal measures, such as cluster stability, rely only on the data and the
  result themselves. Currently there are no internal bicluster measures in
  scikit-learn. External measures refer to an external source of information,
  such as the true solution.
* To compare a set of found biclusters to the set of true biclusters, two
  similarity measures are needed: a similarity measure for individual
  biclusters, and a way to combine these individual similarities into an overall
  score.
* To compare individual biclusters, several measures have been used. For now,
  only the Jaccard index is implemented:
  ```
  J(A,B) = |A \intersection B| / (|A| + |B| - |A \intersection B|)
  ```
    - The Jaccard index achieves its minimum of 0 when the biclusters to not
      overlap at all and its maximum of 1 when they are identical.
* Several methods have been developed to compare two sets of biclusters. For now, only consensus_score (Hochreiter et. al., 2010) is available:
    1. Compute bicluster similarities for pairs of biclusters, one in each set, using the Jaccard index or a similar measure.
    1. Assign biclusters from one set to another in a one-to-one fashion to maximize the sum of their similarities. This step is performed using the Hungarian algorithm.
    1. The final sum of similarities is divided by the size of the larger set.
The minimum consensus score, 0, occurs when all pairs of biclusters are totally
dissimilar. The maximum score, 1, occurs when both sets are identical.l

### Matrix factorization problems, a.k.a., decomposing signals in components

### PCA

* PCA centers but does not scale the input data for each feature before applying the SVD.
* The optional argument `whiten=True` yields principal components scaled to have
  unit variance, which is useful in case of using some down-stream models, e.g.,
  SVM with RBF or K-Means, which assume that signals are isotropic.
* The PCA object also provides a probabilistic interpretation of the PCA that
  can give a likelihood of data based on the amount of variance it explains. As
  such it implements a score method that can be used in cross-validation. See
  http://www.miketipping.com/papers/met-mppca.pdf for more details
*  IncrementalPCA makes it possible to implement out-of-core Principal Component Analysis either by:
    - Using its partial_fit method on chunks of data fetched sequentially from the local hard drive or a network database.
    - Calling its fit method on a memory mapped file using numpy.memmap.
* IncrementalPCA only stores estimates of component and noise variances, in order update explained_variance_ratio_ incrementally. This is why memory usage depends on the number of samples per batch, rather than the number of samples to be processed in the dataset.
* Using the optional argument `svd_solver='randomized'` limits the computation to
  an approximate estimate of the number of principal components we want to keep
  and thereby reduces the time complexity of PCA from `O(n_max^2 n_min)` to
  `O(n_max^2 n_components)` where `n_max, n_min` are the max and min of
  `(n_samples, n_features)` resp. and space complexity is reduced from `n_max *
  n_min` to `n_max * n_components`
    - the implementation of `inverse_transform` in PCA with
      `svd_solver='randomized'` is not the exact inverse transform of transform
      even when `whiten=False` (default).
* `KernelPCA` achieves non-linear dimensionality reduction by using a kernel,
  e.g., pairwise metrics, affinities & kernels and is useful for denoising,
  compression and structured prediction (kernel dependency estimation).
* Principal component analysis (PCA) has the disadvantage that the components extracted by this method have exclusively dense expressions, i.e. they have non-zero coefficients when expressed as linear combinations of the original variables. This can make interpretation difficult. In many cases, the real underlying components can be more naturally imagined as sparse vectors; for example in face recognition, components might naturally map to parts of faces.
* SparsePCA is a variant of PCA, with the goal of extracting the set of sparse components that best reconstruct the data.  Mini-batch sparse PCA (MiniBatchSparsePCA) is a variant of SparsePCA that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.

#### Truncated SVD and latent semantic analysis

* TruncatedSVD implements a variant of singular value decomposition (SVD) that
  only computes the k largest singular values, where k is a user-specified parameter.
* When truncated SVD is applied to term-document matrices (as returned by
  CountVectorizer or TfidfVectorizer), this transformation is known as latent
  semantic analysis (LSA) or latent semantic indexing (LSI), because it transforms such matrices to a "semantic" space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.
* Mathematically, truncated SVD applied to training samples  produces a low-rank
  approximation:
  ```
  X \approx X_k = U_k \Sigma_k V_k^T
  ```
  and the transformed training set given by `n_components` is `U_k \Sigma_k`.
  Transforming a test set X is done by: `X' = X  V_k`
* TruncatedSVD is very similar to PCA, but differs in that it works on sample matrices  directly instead of their covariance matrices. When the columnwise (per-feature) means of  are subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCA. In practical terms, this means that the TruncatedSVD transformer accepts scipy.sparse matrices without the need to densify them, as densifying may fill up memory even for medium-sized document collections.

### Dictionary learning

* The SparseCoder object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a fit method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the transform_method initialization parameter:
    - Orthogonal matching pursuit (Orthogonal Matching Pursuit (OMP))
    - Least-angle regression (Least Angle Regression)
    - Lasso computed by least-angle regression
    - Lasso using coordinate descent (Lasso)
    - Thresholding
* Dictionary learning (DictionaryLearning) is a matrix factorization problem that amounts to finding a (usually overcomplete) dictionary that will perform well at sparsely encoding the fitted data.  Representing data as sparse combinations of atoms from an overcomplete dictionary is suggested to be the way the mammalian primary visual cortex works. Consequently, dictionary learning applied on image patches has been shown to give good results in image processing tasks such as image completion, inpainting and denoising, as well as for supervised recognition tasks.  Dictionary learning is an optimization problem solved by alternatively updating the sparse code, as a solution to multiple Lasso problems, considering the dictionary fixed, and then updating the dictionary to best fit the sparse code. 

### Factor analysis

* Assume that the dataset `X ={x_1, ..., x_n}` can be described in terms of a
  continuous latent variable `h` as:
  ```
  x_i = Wh_i + \mu + \epsilon
  ```
  where `\epsilon` is a noise term distributed according to a Gaussian
  distribution `\epsilon ~ N(0, \Psi)` and `\mu` is an arbitrary offset vector.
    - Also called a "generative" model.
* Collecting all the x's into columns of the matrix X, we can decompose it as:
```
X = WH + M + E
```
* The above equation implies the foll. probabilistic interpretation:
```
p(x_i | h_i) = N(WH + \mu_i, \Psi)
```
* To complete the probabilistic description, a prior distribution on `h` is
  required and it is taken to be `h ~ N(0, I)`, which yields  a Gaussian for the
  marginal distribution for x:
  ```
  p(x) = N(\mu, WW^T + \Psi)
  ```
* 2 more assumptions are made for `\Psi`:
    - `\Psi = \sigma^2 I`, leads to probabilistic PCA
    - `\Psi - diag(\psi_1,...\psi_n)`, leads to factor analysis
    - Thus, Both models essentially estimate a Gaussian with a low-rank covariance matrix.
* The main advantage for Factor Analysis over PCA is that it can model the
  variance in every direction of the input space independently (heteroscedastic
  noise), which allows better model selection than probabilistic PCA in the
  presence of heteroscedastic noise.

### Independent component analysis (ICA)

* Independent component analysis separates a multivariate signal into additive
  subcomponents that are maximally independent.
* Typically, ICA is not used for reducing dimensionality but for separating
  superimposed signals ("blind source separation").
* Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied.
* ICA can also be used as yet another non linear decomposition that finds
  components with some sparsity.

### Non-negative matrix factorization (NMF)

* Assumes that the data and its components are non-negative.
* Drop-in replacement for PCA or its variants when data doesn't contain negative
  values.
* Essentially finds a multiplicative decomposition of X into 2 W & H of
  non-negative elements, by optimizing the distance (typically Froebenius norm)
  between X and its multiplicative decomposition.
* Unlike PCA, the representation of a vector is obtained in an additive fashion,
  by superimposing the components, without subtracting. Such additive models are
  efficient for representing images and text.
* Can also add elastic net style L1 & L2 regularization terms for both W & H
  to the cost function.
* Besides Froebenius distance, other distances can also be used in the cost
  function, all of which are special cases of the beta distance:
  ```
  d_\beta(X,Y) = \sum_{i,j} 1/(\beta (\beta -1) ) (X_{ij}^\beta + (\beta-1)
  Y_{ij} - \beta X_{ij} Y_{ij}^{\beta -1} )
  ```

### Latent Dirichlet Allocation (LDA)

* Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora.
* It is also a topic model that is used for discovering abstract topics from a collection of documents.
* The graphical model of LDA is a three-level generative model (see
https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda
for graphical representation)
* When modeling text corpora, the model assumes the foll. generative process for
  a corpus with D documents and K topics (K corresponds to n_components in the
  API):
    1. For each topic `k \in K`, draw `\beta_k ~ Dirichlet(\eta)`. This provides
      a distribution over the words, i.e. the probability of a word appearing in
      topic k.  corresponds to `topic_word_prio`.
    1. For each document `d i\in D`, draw the topic proportions `\theta_d ~
      Dirichlet(\alpha)`. `\alpha` corresponds to `doc_topic_prior`
    1. For each word i in document d:
        i. Draw the topic assignment `z_{di} ~ Multinomial(\theta_d)`
        i. Draw the observed word `w_{ij} ~ Multinomial(\beta_{z_{di}})`
  For parameter estimation, the posterior distribution:
  ```
  p(z, \theta, \beta | w, \alpha, \eta) = p(z, \theta, \beta |  \alpha, \eta) /
  p(w | \alpha, \eta)
  ```
  is untractable, so variational Bayes method uses a simpler distribution as an
  approximation and uses it to maximize the Evidence Lower Bound (ELBO), as
  maximizing it is equivalent to minimizing KL-divergence between p & q.
* When LatentDirichletAllocation is applied on a “document-term” matrix, the
  matrix will be decomposed into a “topic-term” matrix and a “document-topic”
  matrix. While “topic-term” matrix is stored as `components_` in the model,
  “document-topic” matrix can be calculated from transform method.

## Covariance estimation

* For covariance estimation, which can be seen as estimation of the data set
  scatter plot shape, it is assumed that the observations are i.i.d.

### Empirical covariance

* The Maximum Likelihood Estimator of a sample is an unbiased estimator of the
  corresponding population's covariance matrix, which can be computed using the`empirical_covariance` function of the package, or by fitting an `EmpiricalCovariance` object to the data sample with the `EmpiricalCovariance.fit` method.
* The results of empirical covariance depend on whether the data is
  centered or not:
    -  If `assume_centered=False`, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and `assume_centered=True` should be used.

### Shrunk covariance

* Despite being an unbiased estimator of the covariance matrix, the Maximum Likelihood Estimator is not a good estimator of the eigenvalues of the covariance matrix, so the precision matrix obtained from its inversion is not accurate. Sometimes, it even occurs that the empirical covariance matrix cannot be inverted for numerical reasons. To avoid such an inversion problem, a transformation of the empirical covariance matrix has been introduced: the shrinkage.
* Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation:
```
\Sigma_shrunk = (1 - \alpha) \hat{\Sigma} + \alpha Tr \hat{\Sigma}/p Id
```
    - Choosing the amount of shrinkage,  amounts to setting a bias/variance
      trade-off.

#### Ledoit-Wolf shrinkage

* In their 2004 paper, O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient  that minimizes the Mean Squared Error between the estimated and the real covariance matrix.

#### Oracle Approximating Shrinkage

* Under the assumption that the data are Gaussian distributed, Chen et al.  derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance.

### Sparse inverse covariance

* The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as covariance selection.

* In the small-samples situation, in which `n_samples` is on the order of
  `n_features` or smaller, sparse inverse covariance estimators tend to work
  better than shrunk covariance estimators. However, in the opposite situation,
  or for very correlated data, they can be numerically unstable. In addition,
  unlike shrinkage estimators, sparse estimators are able to recover
  off-diagonal structure.

* The GraphicalLasso estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its alpha parameter, the more sparse the precision matrix. The corresponding GraphicalLassoCV object uses cross-validation to automatically set the alpha parameter.
```
\hat{K} = argmin_K(tr SK - log det K + \alpha ||K||_1)
```
where K is the precision matrix, S is the sample covariance matrix and `||K||_1`
is the sum of the absolute values of the off-diagonal coefficients of K.

### Robust covariance estimation

* The empirical covariance estimator and the shrunk covariance estimators
  presented above are very sensitive to the presence of outliers in the data.
* Also, robust covariance estimators can be used to perform outlier detection and discard/downweight some observations according to further processing of the data.
* The Minimum Covariance Determinant estimator is a robust estimator of a data set’s covariance introduced by P.J. Rousseeuw. The idea is to find a given proportion (h) of "good" observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations ("consistency step"). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set ("reweighting step").
* The sklearn implementation is based on the FastMCD algorithm, which also computes a robust estimate of the data set location at the same time.

## Novelty and outlier detection

* Outlier detection and novelty detection are both used for anomaly detection, where one is interested in detecting abnormal or unusual observations. Outlier detection is then also known as unsupervised anomaly detection and novelty detection as semi-supervised anomaly detection. In the context of outlier detection, the outliers/anomalies cannot form a dense cluster as available estimators assume that the outliers/anomalies are located in low density regions. On the contrary, in the context of novelty detection, novelties/anomalies can form a dense cluster as long as they are in a low density region of the training data, considered as normal in this context.
* Inliers are labeled 1, while outliers are labeled -1. The predict method makes use of a threshold on the raw scoring function computed by the estimator. This scoring function is accessible through the `score_samples` method, while the threshold can be controlled by the contamination parameter.  The `decision_function` method is also defined from the scoring function, in such a way that negative values are outliers and non-negative ones are inliers.
* The svm.OneClassSVM is known to be sensitive to outliers and thus does not perform very well for outlier detection.
* covariance.EllipticEnvelope assumes the data is Gaussian and learns an ellipse.

### Novelty detection

* In novelty detection, we try to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding space.
* Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.
* The One-Class SVM has been introduced by Schölkopf et al. for that purpose and implemented in the Support Vector Machines module in the svm.OneClassSVM object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The  parameter `\nu`, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.

### Outlier detection

* Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations from some polluting ones, called outliers. Yet, in the case of outlier detection, we don’t have a clean data set representing the population of regular observations that can be used to train any tool.

#### Fitting an elliptic envelope

* One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the "shape" of the data, and can define outlying observations as observations which stand far enough from the fit shape.

* The scikit-learn provides an object `covariance.EllipticEnvelope` that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.

* For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness.
```
Mahalanobis distance between an observation x and a set of observations with
mean \mu and covariance matrix S = \sqrt{(x-\mu)^T S^{-1} (x-\mu)}
```
    - Mahalanobis distance is thus multi-dimensional generalization of the idea
      of measuring how many standard deviations away is an observation from the
      mean of a distribution.

#### Isolation Forest

* One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The ensemble.IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.
* Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.
This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.
* Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.
* The implementation of ensemble.IsolationForest is based on an ensemble of tree.ExtraTreeRegressor. Following Isolation Forest original paper, the maximum depth of each tree is set to `ceil(log_2(n))` where  n is the number of samples used to build the tree.

#### Local outlier factor

* The neighbors.LocalOutlierFactor (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.
* In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.
* The number k of neighbors considered, (alias parameter `n_neighbors`) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking `n_neighbors=20` appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), `n_neighbors` should be greater.
* The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.
* When applying LOF for outlier detection, there are no `predict, decision_function and score_samples` methods but only a `fit_predict` method. The scores of abnormality of the training samples are accessible through the `negative_outlier_factor_` attribute. Note that predict, `decision_function` and `score_samples` can be used on new unseen data when LOF is applied for novelty detection, i.e. when the novelty parameter is set to True.
* To use neighbors.LocalOutlierFactor for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the novelty parameter set to True before fitting the estimator.
    - `fit_predict` is not available in this case.
    - When novelty is set to True be aware that you must only use predict, `decision_function` and `score_samples` on new unseen data and not on the training samples as this would lead to wrong results. The scores of abnormality of the training samples are always accessible through the `negative_outlier_factor_` attribute.

## Density estimation

* Density estimation walks the line between unsupervised learning, feature
  engineering, and data modeling.
* Density estimation is a very simple concept, and most people are already
  familiar with one common density estimation technique: the histogram.
    - A major problem with histograms, however, is that the choice of binning
      can have a disproportionate effect on the resulting visualization.
* If, instead of binning the data to produce a histogram, each data sample was
  indicated with a point in the plot, we obtain a much more accurate
  representation of the data, but the plot may not be very smooth. This is an
  example of kernel density estimation, using, in this case, a tophat kernel.
* Kernel density estimation in scikit-learn is implemented in the
  sklearn.neighbors.KernelDensity estimator, which uses the Ball Tree or KD Tree
  for efficient queries.
* Kernel density estimation can be performed in any number of dimensions, though
  in practice the curse of dimensionality causes its performance to degrade in
  high dimensions.
* Mathematically, a kernel is a positive function K(x; h), which is controlled
  by a bandwidth parameter h and defines the density estimate at a point y
  within a group of points xi as:
  ```
  \rho_K(y) = \sum_i K((y-xi)/h)
  ```
* The bandwidth h acts a smoothing parameter, controlling the bias-variance
  tradeoff, since a small h leads to a high-variance unsmooth density
  distribution and a large h leads to a high-bias very smooth density
  distribution.
* The kernels implemented by `sklearn.neighbors.KernelDensity` are:
    - `kernel='gaussian': K(x;h) \prop exp(-x^2/2h^2)`
    - `kernel='tophat': K(x;h) \prop 1 if |x| < h, 0 otherwise`
    - `kernel='epanechnikov': K(x;h) \prop 1 - x^2/h^2 if |x| < h, 0 otherwise`
    - `kernel='exponential': K(x;h) \prop exp(-x/h)`
    - `kernel='linear': K(x;h) \prop 1 - x/h if |x| < h, 0 otherwise`
    - `kernel='cosine': K(x;h) \prop cos(\pi*x/(2*h)) if |x| < h, 0 otherwise`
* The kernel density estimator can be used with any of the valid distance metrics (see sklearn.neighbors.DistanceMetric for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the Haversine distance which measures the angular distance between points on a sphere.
* One other useful application of kernel density estimation is to learn a non-parametric generative model of a dataset in order to efficiently draw new samples from this generative model.

## Neural network models (unsupervised)

### Restricted Boltzmann Machines

* Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model. The features extracted by an RBM or a hierarchy of RBMs often give good results when fed into a linear classifier such as a linear SVM or a perceptron.
* The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides BernoulliRBM, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.
    - The conditional probability distribution of each unit is given by the
      logistic sigmoid activation function of the input it receives:
      ```
      P(vi = 1 | h) = \sigma(\sum_j wij hj + b)
      P(hi = 1 | v) = \sigma(\sum_i wij vi + c)
      ```
* The RBM tries to maximize the likelihood of the data using a particular graphical model: a fully-connected bipartite graph with all hidden nodes connected to all visible nodes.
    - The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit.
* The energy function measures the quality of a joint assignment of weights wij
  between hidden nodes hj and visible nodes vi as:
```
E(v,h) = \sum_ij wij hi vj - \sum_i bi vi - \sum_j cj hj
```
where b & c are the intercept vectors for the visible and hidden layers resp.
    - The joint probability of the model is defined as:
    ```
    P(v,h)  = exp(-E(v,h))/Z
    ```
* The word restricted refers to the bipartite structure of the model, which
  prohibits direct interaction between hidden units, or between visible units,
  which allows for the use of efficient block Gibbs sampling for inference.
* The parameter learning algorithm used (Stochastic Maximum Likelihood) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.
    - Optimizing maximum likelihood directly is infeasible because of the form
      of the data likelihood.
      ```
      log P(v) = log \sum_h exp(-E(v,h)) - log \sum_{x,y} exp(-E(x,y))
      ```
    - The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples.
    - In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of v and h  given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes.
    - The Contrastive Divergence method suggests to stop the chain after a small number of iterations, , usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.
    - Persistent Contrastive Divergence addresses this. Instead of starting a new chain each time the gradient is needed, and performing only one Gibbs sampling step, in PCD we keep a number of chains (fantasy particles) that are updated  Gibbs steps after each weight update. This allows the particles to explore the space more thoroughly.
* The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.

# Model selection and evaluation

## Cross-validation

* Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set
* In scikit-learn a random split into training and test sets can be quickly computed with the `train_test_split` helper function.
* The simplest way to use cross-validation is to call the `cross_val_score` helper function on the estimator and the dataset.
```
>>> from sklearn.model_selection import cross_val_score
>>> clf = svm.SVC(kernel='linear', C=1)
>>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)
>>> scores
array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])
# The mean score and the 95% confidence interval of the score estimate are hence given by:
>>> print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
Accuracy: 0.98 (+/- 0.03)
```
* By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter
* When the cv argument is an integer, `cross_val_score` uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.
* It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:
```
>>> from sklearn.model_selection import ShuffleSplit
>>> n_samples = iris.data.shape[0]
>>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
>>> cross_val_score(clf, iris.data, iris.target, cv=cv)  
array([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])
```
* Another option is to use an iterable yielding (train, test) splits as arrays of indices, for example:
```
>>> def custom_cv_2folds(X):
...     n = X.shape[0]
...     i = 1
...     while i <= 2:
...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)
...         yield idx, idx
...         i += 1
...
>>> custom_cv = custom_cv_2folds(iris.data)
>>> cross_val_score(clf, iris.data, iris.target, cv=custom_cv)
array([1.        , 0.973...])
```
* Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar data transformations similarly should be learnt from a training set and applied to held-out data for prediction
* The `cross_validate` function differs from `cross_val_score` in two ways:
    i. It allows specifying multiple metrics for evaluation.
    i. It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.
* The function `cross_val_predict` has a similar interface to `cross_val_score`, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).
    - The result of `cross_val_predict` may be different from those obtained using `cross_val_score` as the elements are grouped in different ways. The function `cross_val_score` takes an average over cross-validation folds, whereas `cross_val_predict` simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, `cross_val_predict` is not an appropriate measure of generalisation error.
* Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples. Only then, one can use CV strategies like Kfold.
* `KFold` splits dataset into k consecutive folds (without shuffling by default), where the first `n_samples % n_splits` folds have size `n_samples // n_splits + 1`, other folds have size `n_samples // n_splits`, where `n_samples` is the number of samples.
* When one requires to run KFold n times, producing different splits in each repetition, use `RepeatedKFold`.
    - Similarly, RepeatedStratifiedKFold repeats Stratified K-Fold n times with different randomization in each repetition.
* In terms of accuracy, `LeaveOneOut` (LOO) often results in high variance as an estimator for the test error. Intuitively, since  of the samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.
However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.
As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO.
* LeavePOut is very similar to LeaveOneOut as it creates all the possible training/test sets by removing  p samples from the complete set.
    - Unlike LeaveOneOut and KFold, the test sets will overlap for p > 1.
* The ShuffleSplit iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.
    - It is possible to control the randomness for reproducibility of the results by explicitly seeding the `random_state` pseudo random number generator.
    - ShuffleSplit is thus a good alternative to KFold cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split.
* StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.
    - RepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times with different randomization in each repetition.
    - StratifiedShuffleSplit is a variation of ShuffleSplit, which returns stratified splits, i.e which creates splits by preserving the same percentage for each target class as in the complete set.
* The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.
Such a grouping of data is domain specific. An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier.
In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold.
* GroupKFold is a variation of k-fold which ensures that the same group is not represented in both testing and training sets. For example if the data is obtained from different subjects with several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. GroupKFold makes it possible to detect this kind of overfitting situations.
    - The folds may not have exactly the same size due to the imbalance in the data.
* LeaveOneGroupOut is a cross-validation scheme which holds out the samples according to a third-party provided array of integer groups.
    - Each training set is thus constituted by all the samples except the ones related to a specific group.
    - the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.
    - LeavePGroupsOut is similar as LeaveOneGroupOut, but removes samples related to P groups for each training/test set.
    - The GroupShuffleSplit iterator behaves as a combination of ShuffleSplit and LeavePGroupsOut, and generates a sequence of randomized partitions in which a subset of groups are held out for each split.
        * This class is useful when the behavior of LeavePGroupsOut is desired, but the number of groups is large enough that generating all possible partitions with P groups withheld would be prohibitively expensive. In such a scenario, GroupShuffleSplit provides a random sample (with replacement) of the train / test splits generated by LeavePGroupsOut.
* Time series data is characterised by the correlation between observations that are near in time (autocorrelation). However, classical cross-validation techniques such as KFold and ShuffleSplit assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalisation error) on time series data. Therefore, it is very important to evaluate our model for time series data on the “future” observations least like those that are used to train the model. To achieve this, one solution is provided by `TimeSeriesSplit`.
    - TimeSeriesSplit is a variation of k-fold which returns first k folds as train set and the (k+1)-th fold as test set. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model.  This class can be used to cross-validate time series data samples that are observed at fixed time intervals.

## Tuning the hyper-parameters of an estimator

* Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes.
* It is possible and recommended to search the hyper-parameter space for the best cross validation score.
* To find the names and current values for all parameters for a given estimator, use `estimator.get_params()`
* It is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.
* The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the `param_grid` parameter.
* The GridSearchCV instance implements the usual estimator API: when “fitting” it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.
*  RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:
    - A budget can be chosen independent of the number of parameters and possible values.
    - Adding parameters that do not influence the performance does not decrease efficiency.
    - Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the `n_iter` parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified.
    - For continuous parameters,  it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing `n_iter` will always lead to a finer search.
* By default, parameter search uses the score function of the estimator to evaluate a parameter setting. These are the `sklearn.metrics.accuracy_score` for classification and `sklearn.metrics.r2_score` for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the `scoring` parameter to GridSearchCV, RandomizedSearchCV.
* GridSearchCV and RandomizedSearchCV allow specifying multiple metrics for the scoring parameter.
Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s).
    - When specifying multiple metrics, the refit parameter must be set to the metric (string) for which the `best_params_` will be found and used to build the `best_estimator_` on the whole dataset. If the search should not be refit, set `refit=False`. Leaving refit to the default value None will result in an error when using multiple metrics.
* Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to "train" the parameters of the grid.
* Since GridSearchCV and RandomizedSearchCV evaluate each parameter setting independently, computations can be run in parallel if your OS supports it, by using the keyword `n_jobs=-1`
* Some parameter settings may result in a failure to fit one or more folds of the data. By default, this will cause the entire search to fail, even if some parameter settings could be fully evaluated. Setting `error_score=0` (or `=np.NaN`) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to 0 (or NaN), but completing the search.
* Some models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter. This feature can be leveraged to perform a more efficient cross-validation used for model selection of this parameter.
    - The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the regularization path of the estimator.
    - This is available for:
        * `linear_model.ElasticNetCV([l1_ratio, eps, …])`
        * `linear_model.LarsCV([fit_intercept, …])`
        * `linear_model.LassoCV([eps, n_alphas, …])`
        * `linear_model.LassoLarsCV([fit_intercept, …])`
        * `linear_model.LogisticRegressionCV([Cs, …])`
        * `linear_model.MultiTaskElasticNetCV([…])`
        * `linear_model.MultiTaskLassoCV([eps, …])`
        * `linear_model.OrthogonalMatchingPursuitCV([…])`
        * `linear_model.RidgeCV([alphas, …])`
        * `linear_model.RidgeClassifierCV([alphas, …])`
* Some models can offer an information-theoretic closed-form formula of the optimal estimate of the regularization parameter by computing a single regularization path (instead of several when using cross-validation).
    - `linear_model.LassoLarsIC([criterion, …])`
* When using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out.
    - This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes “for free” as no additional data is needed and can be used for model selection.
    - Available for:
        * `ensemble.RandomForestClassifier([…])`
        * `ensemble.RandomForestRegressor([…])`
        * `ensemble.ExtraTreesClassifier([…])`
        * `ensemble.ExtraTreesRegressor([n_estimators, …])`
        * `ensemble.GradientBoostingClassifier([loss, …])`
        * `ensemble.GradientBoostingRegressor([loss, …])`

## Model evaluation

* 3 main ways to evaluate quality of a model's predictions:
    i. The default evaluation criterion provided by the `score` method of each
    estimator.
    i. The internal scoring strategy used by model evaluation tools using
    cross-validation, which can be customized by using the `scoring` parameter.
    i. Functions in the metrics module.
        - The module sklearn.metrics also exposes a set of simple functions measuring a prediction error given ground truth and prediction:
        * functions ending with `_score` return a value to maximize, the higher the better.
        * functions ending with `_error` or `_loss` return a value to minimize, the lower the better. When converting into a scorer object using `make_scorer`, set the greater_is_better parameter to False (True by default).
* The simplest way to generate a callable object for scoring is by using `make_scorer`. That function converts metrics into callables that can be used for model evaluation.
* You can define your own callable to be a scorer, provided it needs to meet the protocol specified by the following two rules:
    1. It can be called with parameters (estimator, X, y), where estimator is the model that should be evaluated, X is validation data, and y is the ground truth target for X (in the supervised case) or None (in the unsupervised case).
    1. It returns a floating point number that quantifies the estimator prediction quality on X, with reference to y. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.
        - While defining the custom scoring function alongside the calling function should work out of the box with the default joblib backend (loky), importing it from another module will be a more robust approach and work independently of the joblib backend.
* Scikit-learn also permits evaluation of multiple metrics in GridSearchCV, RandomizedSearchCV and `cross_validate`.
    - There are two ways to specify multiple scoring metrics for the scoring parameter:
        i. As an iterable of string metrics
        i. As a dict mapping the scorer name to the scoring function, where the
        dict values can either be scorer functions or one of the predefined
        metric strings.
            - Currently only those scorer functions that return a single score
              can be passed inside the dict. Scorer functions that return
              multiple values are not permitted and will require a wrapper to
              return a single metric.
* Some metrics are essentially defined for binary classification tasks (e.g. `f1_score, roc_auc_score`). In these cases, by default only the positive label is evaluated, assuming by default that the positive class is labelled 1 (though this may be configurable through the `pos_label` parameter).

### Classification metrics

* In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the `average` parameter:
    - "macro" simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.
    - "weighted" accounts for class imbalance by computing the average of binary metrics in which each class's score is weighted by its presence in the true data sample.
    - "micro" gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.
    - "samples" applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes for each sample in the evaluation data, and returning their (`sample_weight`-weighted) average.
    - Selecting `average`=None will return an array with the score for each class.
* While multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell [i, j] has value 1 if sample i has label j and value 0 otherwise.
* The `accuracy_score` function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions
    - In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.
* The `balanced_accuracy_score` function computes the balanced accuracy, which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy.
    - In the binary case, balanced accuracy is equal to the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate), or the area under the ROC curve with binary predictions rather than scores.
    - If the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to `1/n_classes`.
    - The score ranges from 0 to 1, or when adjusted=True is used, it rescaled to the range  `1/(1-n_classes)` to 1, inclusive, with performance at random scoring 0.
    - With adjusted=True, balanced accuracy reports the relative increase from `1/n_classes`. In the binary case, this is also known as "Youden’s J statistic", or "informedness".
* The function `cohen_kappa_score` computes Cohen's kappa statistic (`1 - (1-Pr(observed agreement))/(1 - Pr(expected agreement))`). This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.
    - The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).
    - Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.
* The `confusion_matrix` function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class.
* The `classification_report` function builds a text report showing the main classification metrics with a custom `target_names` parameters.
* The `hamming_loss` computes the average Hamming loss or Hamming distance between two sets of samples, where the Hamming distance between each pair of samples is averaged over the number of labels.
* Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples.
* The `average_precision_score` function computes the approximate area under the
  precision-recall curve using the rectangular rule.
* In a binary classification task, the terms "positive" and "negative" refer to the classifier's prediction, and the terms "true" and "false" refer to whether that prediction corresponds to the external judgment (sometimes known as the "observation").
* Precision = tp/(tp+fp)
* Recall = tp/(tp+fn)
* `F_\beta = (1+\beta^2) precision x recall / (recall + \beta^2 * precision)`
* In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the `average` argument to the `average_precision_score` (multilabel only), `f1_score`, `fbeta_score`, `precision_recall_fscore_support`, `precision_score` and `recall_score` functions, as described above.
    - Note that if all labels are included, "micro"-averaging in a multiclass setting will produce precision, recall and F that are all identical to accuracy.
    - Also note that "weighted" averaging may produce an F-score that is not between precision and recall.
* For multiclass classification with a "negative class", it is possible to exclude some labels as:
```
>>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')
```
* Similarly, labels not present in the data sample may be accounted for in macro-averaging as:
```
>>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')
```
* The `jaccard_score` function computes the average of Jaccard similarity coefficients, also called the Jaccard index, between pairs of label sets as
```
J(yi, pi) = |yi \intersection pi| / |yi \union pi|
```
where yi & pi are the ground-truth label set and predicted label set for the
i-th samples.
* The `hinge_loss` function computes the average distance between predicted and
  actual labels using hinge loss `|1-wy|_+`, where w is the predicted value and
  y is the actual label (must be one of +1 or -1) and `x_+` denotes the positive
  part of x, i.e, max(0,x).
    - For multiclass, the hingle loss is modified to max{1+yt-yw, 0}, where yw
      is the predicted value for the class with the highest score and yt is the
      max. of the predictions for the other classes.
* The log loss (`sklearn.metrics.log_loss`) is simply the negative loss-likelihood of the probability score
  given the true label, i.e.,
  ```
  L_log(y,p) = -log Pr(y|p) = - y log(p) - (1-y) log(1-p)
  ```
    - For multiclass classification with K classes, this easily generalizes to:
    ```
    L_log(y,p) = - \sum_k=0^K-1 yk log(pk)
    ```
* `matthews_corrcoef` computes Matthew's correlation coefficient (in essence, a
  correlation coefficient between -1 to +1) for binary classes, defined as:
  ```
  MCC = (tp x tn - fp x fn) / \sqrt{(tp+fp) (tp+fn) (tn+fp) (tn+fn)}
  ```
    - can be generalized to multiclass case as:
    ```
    MCC = (c x s - \sum_k pk x tk) / \sqrt{(s^2 - \sum_k pk^2) (s^2 - \sum_k
    tk^2)}
    ```
    where tk & pk are the number of times class k truly occurred and was
    predicted respectively and s & c are the total number of samples and total
    number correctly predicted respectively.
        * MCC is no longer between -1 & 1, the min. is somewhere between -1 & 0
          depending on no. & distribution of ground truth labels, but max. is
          +1.
* The `multilabel_confusion_matrix` function computes class-wise (default) or sample-wise (samplewise=True) multilabel confusion matrix to evaluate the accuracy of a classification. It also treats multiclass data as if it were multilabel, as this is a transformation commonly applied to evaluate multiclass problems with binary classification metrics (such as precision, recall, etc.).
    - When calculating class-wise multilabel confusion matrix C, the count of true negatives for class i is C[i,0,0], false negatives is C[i,1,0], true positives is C[i,1,1] and false positives is C[i,0,1].
        * This is consistent with the binary classification confusion matrix!
* Compared to metrics such as the subset accuracy, the Hamming loss, or the F1 score, ROC doesn't require optimizing a threshold for each label. The `roc_auc_score` function can also be used in multi-class classification, if the predicted outputs have been binarized.
    - In applications where a high false positive rate is not tolerable the parameter `max_fpr` of `roc_auc_score` can be used to summarize the ROC curve (which can be created using the output of the function `roc_curve`) up to the given limit.
* In multilabel classification, the `zero_one_loss` (defined as a 1 whenever there's a mismatch in predicted and actual labels and 0 otherwise) scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors.
* The `brier_score_loss` function returns a score of the mean square difference between the actual outcome and the predicted probability of the possible outcome. The actual outcome has to be 1 or 0 (true or false), while the predicted probability of the actual outcome can be a value between 0 and 1.
    - It can be thought of as a measure of the “calibration” of a set of
      probabilistic predictions.
    - It is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes.

### Multilabel ranking metrics

* The `coverage_error` function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.
* The `label_ranking_average_precision_score` function implements label ranking average precision (LRAP). This metric is linked to the `average_precision_score` function, but is based on the notion of label ranking instead of precision and recall.
    - Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal rank.
* The `label_ranking_loss` function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero.

### Regression metrics

* `mean_squared_error, mean_absolute_error, explained_variance_score and r2_score` have an multioutput keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is `'uniform_average'`, which specifies a uniformly weighted mean over outputs. If an ndarray of shape `(n_outputs,)` is passed, then its entries are interpreted as weights and an according weighted average is returned. If multioutput is `'raw_values'` is specified, then all unaltered individual scores or losses will be returned in an array of shape `(n_outputs,)`.
* The `r2_score` and `explained_variance_score` accept an additional value `'variance_weighted'` for the multioutput parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on well explaining the higher variance variables. `multioutput='variance_weighted'` is the default value for `r2_score` for backward compatibility. This will be changed to `uniform_average` in the future.
* The `explained_variance_score` computes the explained variance regression score between actual values y and predicted values p as `1 - Var{y-p}/Var{y}`.
    - For linear regression, this is the same as the coefficient of
      determination.
* The `max_error` function computes the maximum residual error , a metric that captures the worst case error between the predicted value and the true value.
* The `mean_absolute_error` function computes mean absolute error, a risk metric corresponding to the expected value of the absolute error loss or l1-norm loss.
* The `mean_squared_error` function computes mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.
* The `mean_squared_log_error` function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss.
    - This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc.
    - Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.
* The `median_absolute_error` is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.
* The `r2_score` function computes the coefficient of determination, usually denoted as R^2.
    - As variance is dataset dependent, R^2 may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.

### Dummy estimators

* When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb.
* Note that with all these strategies, the `predict` method completely ignores the input data!
* `DummyClassifier` implements several such simple strategies for classification:
    - `stratified` generates random predictions by respecting the training set class distribution.
    - `most_frequent` always predicts the most frequent label in the training set.
    - `prior` always predicts the class that maximizes the class prior (like `most_frequent`) and `predict_proba` returns the class prior.
    - `uniform` generates predictions uniformly at random.
    - `constant` always predicts a constant label that is provided by the user.
        * A major motivation of this method is F1-scoring, when the positive class is in the minority.
* `DummyRegressor` also implements four simple rules of thumb for regression:
    - `mean` always predicts the mean of the training targets.
    - `median` always predicts the median of the training targets.
    - `quantile` always predicts a user provided quantile of the training targets.
    - `constant` always predicts a constant value that is provided by the user.

## Model persistence

* After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain.
* It is possible to save a model in scikit-learn by using Python’s built-in
  persistence model, namely pickle.
* In the specific case of scikit-learn, it may be better to use joblib's replacement of pickle (dump & load), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string.
* In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:
    - The training data, e.g. a reference to an immutable snapshot
    - The python source code used to generate the model
    - The versions of scikit-learn and its dependencies
    - The cross validation score obtained on the training data

## Validation curves

* The generalization error of any estimator can be decomposed in terms of bias, variance and noise.
    - The bias of an estimator is its average error for different training sets.
    - The variance of an estimator indicates how sensitive it is to varying training sets.
    - Noise is a property of the data.
* The function `validation_curve` is helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values, by providing the training and test scores on each fold of n CV folds.
* A learning curve shows the validation and training score of an estimator for varying numbers of training samples.
    - If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data.
    - If the training score is much greater than the validation score for the
      maximum number of training samples, adding more training samples will most
      likely increase generalization.
* The function `learning_curve` can be used to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets).

# Inspection

## Partial dependence plots

* Partial dependence plots (PDP) show the dependence between the target response  and a set of 'target' features, marginalizing over the values of all other features (the 'complement' features).
    - Intuitively, we can interpret the partial dependence as the expected target response as a function of the 'target' features.
* One-way PDPs tell us about the interaction between the target response and the
  target feature (e.g. linear, non-linear).
* PDPs assume that the target features are independent from the complement features, and this assumption is often violated in practice.
* PDPs with two target features show the interactions among the two features.
* The sklearn.inspection module provides a convenience function `plot_partial_dependence` to create one-way and two-way partial dependence plots.
* For multi-class classification, you need to set the class label for which the
  PDPs should be created via the target argument. The same parameter target is used to specify the target in multi-output regression settings.
* If you need the raw values of the partial dependence function rather than the plots, you can use the `sklearn.inspection.partial_dependence` function
    - The values at which the partial dependence should be evaluated are
      directly generated from X.
    - The values field returned by `sklearn.inspection.partial_dependence` gives the actual values used in the grid for each target feature. They also correspond to the axis of the plots.
    - For each value of the 'target' features in the grid the partial dependence function needs to marginalize the predictions of the estimator over all possible values of the 'complement' features. With the 'brute' method, this is done by replacing every target feature value of X by those in the grid, and computing the average prediction.
    - In decision trees this can be evaluated efficiently without reference to the training data ('recursion' method, where X is only used to generate the grid, not to compute the averaged predictions. The averaged predictions will always be computed on the data with which the trees were trained).

## Pipelines and composite estimators

### Pipeline

* Recommended best practice as they offer the following advantages:
    - Convenience: Need to call `fit` and `predict` only once
      on the data to fit a whole sequence of estimators
    - Joint parameter selection: Allows grid search over parameters for all
      estimators at once
    - Safety & encapsulation: avoid leaking statistics from your test data into
      the trained model in cross-validation, by ensuring that the same samples
      are used to train the transformers and predictors.
* All estimators in a pipeline, except the last one must
  be transformers, i.e., have a `transform` method.
* Calling fit on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it on to the next step.
* The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline.
* The Pipeline is built using a list of (key, value) pairs, where the key is a
  string containing the name you want to give this step and value is an
  estimator object.
* The utility function `make_pipeline` is a shorthand for constructing pipelines;
  it takes a variable number of estimators and returns a pipeline, filling in
  the names automatically.
* The estimators of a pipeline are stored as a list in the steps attribute, but
  can be accessed by index or name.
* Pipeline's `named_steps` attribute allows accessing steps by name with tab
  completion in interactive environments.
* A sub-pipeline can also be extracted using the slicing notation commonly used
  for Python Sequences such as lists or strings (although only a step of 1 is
  permitted).
* Parameters of the estimators in the pipeline can be accessed using the
  `<estimator>__<parameter>` syntax, which is especially important for doing
  grid searches:

```
>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]
>>> pipe = Pipeline(estimators)
>>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],
...                   clf__C=[0.1, 10, 100])
>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)
```

* Individual steps may also be replaced as parameters, and non-final steps may
  be ignored by setting them to `'passthrough'`.

```
>>> from sklearn.linear_model import LogisticRegression
>>> param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],
...                   clf=[SVC(), LogisticRegression()],
...                   clf__C=[0.1, 10, 100])
>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)
```

#### Caching the pipeline

* Since fitting transformers may be computationally expensive, with its memory parameter set, Pipeline will cache each transformer after calling fit.
* This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical, e.g., in the case of a grid search in which the transformers can be fitted only once and reused for each configuration.
* The parameter `memory` is needed in order to cache the transformers, which can be either a string containing the directory where to cache the transformers or a `joblib.Memory` object.
* Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly.

### TransformedTargetRegressor

* `TransformedTargetRegressor` transforms the targets y before fitting a regression model.
* The predictions are mapped back to the original space via an inverse transform.
* It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable.
* The transformation can be triggered by setting either `transformer` or the pair of functions `func` and `inverse_func`. However, setting both options will raise an error.

### FeatureUnion

* FeatureUnion combines several transformer objects into a new transformer that combines their output.
* A FeatureUnion takes a list of transformer objects.
* During fitting, each of these is fit to the data independently.
* The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix.
* When you want to apply different transformations to each field of the data,
  see the related class sklearn.compose.ColumnTransformer.
* FeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and validation.
* A FeatureUnion has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are the caller's responsibility.
* A FeatureUnion is built using a list of (key, value) pairs, where the key is
  the name you want to give to a given transformation (an arbitrary string; it
  only serves as an identifier) and value is an estimator object.
* Like pipelines, feature unions have a shorthand constructor called `make_union` that does not require explicit naming of the components.
* Like Pipeline, individual steps may be replaced using `set_params`, and ignored by setting to `'drop'`.

### ColumnTransformer

* Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps.
* Even though often it is easiest to preprocess data before applying scikit-learn methods, for example using pandas, processing your data before passing it to scikit-learn might be problematic for one of the following reasons:
    1. Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as data leakage), for example in the case of scalers or imputing missing values.
    1. You may want to include the parameters of the preprocessors in a parameter search.
* The ColumnTransformer helps performing different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parametrized.
* ColumnTransformer works on arrays, sparse matrices, and pandas DataFrames.
* To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method
* Apart from a scalar or a single item list, the column selection can be
  specified as a list of multiple items, an integer array, a slice, or a boolean
  mask. Strings can reference columns if the input is a DataFrame, integers are
  always interpreted as the positional columns.
* The `make_column_transformer` function is available to more easily create a ColumnTransformer object. Specifically, the names will be given automatically.

## Feature Extraction

* The class DictVectorizer can be used to convert feature arrays represented as
  lists of standard Python dict objects to the NumPy/SciPy representation used
  by scikit-learn estimators.

  ```
  >>> measurements = [
...     {'city': 'Dubai', 'temperature': 33.},
...     {'city': 'London', 'temperature': 12.},
...     {'city': 'San Francisco', 'temperature': 18.},
... ]

>>> from sklearn.feature_extraction import DictVectorizer
>>> vec = DictVectorizer()

>>> vec.fit_transform(measurements).toarray()
array([[ 1.,  0.,  0., 33.],
       [ 0.,  1.,  0., 12.],
       [ 0.,  0.,  1., 18.]])

>>> vec.get_feature_names()
['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']
  ```
* The class FeatureHasher is a high-speed, low-memory vectorizer that uses a
  technique known as feature hashing, or the “hashing trick”. Instead of
  building a hash table of the features encountered in training, as the
  vectorizers do, instances of FeatureHasher apply a hash function to the
  features to determine their column index in sample matrices directly. The
  result is increased speed and reduced memory usage, at the expense of
  inspectability; the hasher does not remember what the input features looked
  like and has no `inverse_transform` method.

### Text feature extraction

* CountVectorizer implements both tokenization and occurrence counting
  (including of n-grams) in a single class.
* In tf-idf, each term t has it's frequency (tf) is multiplied by the inverse document
  frequency (idf): tf-idf(t,d) = tf(t,d) x idf(t)
    - In academic literature, idf(t) = log (n / (1 + df(t) ) ), but sklearn's 
    TfidfTransformer's default settings have idf(t) = log ((1+n) / (1+df(t))) + 1,
    where df(t) is the number of documents in which term t occurs and n is the
    total number of documents. The resulting tf-idf vector for each document are
    then normalized to have Euclidean norm 1.
* TfidfVectorizer is equivalent to CountVectorizer followed by TfidfTransformer.
* Representation of text in documents in the form of a collection of character
  n-grams is resilient against misspellings and derivations.
* While some local positioning information can be preserved by extracting
  n-grams instead of individual words, bag of words and bag of n-grams destroy
  most of the inner structure of the document and hence most of the meaning
  carried by that internal structure.
    - The task of taking the inner structure into account is outside the scope
      of sklearn & hence, it's not very useful for NLU.
* `HashingVectorizer` allows one to vectorize large datasets of text efficiently
  by using the "hashing trick" from `FeatureHasher`
    - HashingVectorizer is stateless, meaning that you don’t have to call fit on
      it.
    - It is not possible to invert the model (no `inverse_transform` method),
      nor to access the original string representation of the features,
      because of the one-way nature of the hash function that performs the mapping.
    - It does not provide IDF weighting as that would introduce statefulness in the model.
      A TfidfTransformer can be appended to it in a pipeline if required.
    - It can be used with mini-batches to do out-of-core scaling for extremely
      large datasets.

### Image feature extraction

* The `extract_patches_2d` function extracts patches from an image stored as a
  two-dimensional array, or three-dimensional with color information along the third axis.
    - For rebuilding an image from all its patches, use `reconstruct_from_patches_2d`
* The PatchExtractor class works in the same way as `extract_patches_2d`, only
  it supports multiple images as input.
    - It is implemented as an estimator, so it can be used in pipelines.
* The function `img_to_graph` returns a connectivity matrix from a 2D or 3D image.
    - Similarly, `grid_to_graph` build a connectivity matrix for images given the shape of these image.
    - These matrices can be used to impose connectivity in estimators that use
      connectivity information, such as Ward clustering (Hierarchical clustering),
      but also to build precomputed kernels, or similarity matrices.

## Preprocessing data

* The function scale provides a quick and easy way to perform standardization of
  feature variables to 0 mean and unit variance.
    - The corresponding transformer class is StandardScaler.
* MinMaxScaler scales data to lie between a given min. & max. value, often 0 and
  1 respectively.
    - The corresponding function is `minmax_scale`
* MaxAbsScaler scales data so that max. absolute feature value is 1.
    - It is meant for data already centered around 0.
    - The corresponding function is `maxabs_scale`
* MinMaxScaler and MaxAbsScaler are robust to very small standard deviation of
  features.
* MaxAbsScaler and `maxabs_scale` were specifically designed for scaling sparse
  data, and are the recommended way to go about this.
* `scale` and `StandardScaler` can accept scipy.sparse matrices as input, as long as `with_mean=False` is explicitly passed to the constructor.
  Otherwise a ValueError will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally.
*  `robust_scale` and `RobustScaler` are drop-in replacements for standard
   scaling when data contains many outliers as they use more robust estimates for the center and range of your data.
* It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features.
    - To address this issue you can use `sklearn.decomposition.PCA` with `whiten=True` to further remove the linear correlation across features.
* If you have a kernel matrix of a kernel K that computes a dot product in a feature space defined by function phi, a `KernelCenterer` can transform the kernel matrix so that it contains inner products in the feature space defined by phi followed by removal of the mean in that space.
* Two types of transformations are available:
    i. quantile transforms: `QuantileTransformer` and `quantile_transform` provide a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1
    i. power transforms: `PowerTransformer` currently provides two such power transformations, the Yeo-Johnson transform and the Box-Cox transform.
        - Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature.
        -  a quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.
        - Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close
        - Box-Cox power transform can only be applied to strictly positive data
        - It is also possible to map data to a normal distribution using QuantileTransformer by setting `output_distribution='normal'`, whence the median of the data becomes the mean of the transformed data, centered at 0.
* `Normalizer` and the function `normalize` provide a quick and easy way to scale individual samples to have unit norm.
    - The Normalizer transformer class is stateless, so `fit` is a no-op.
    - They can handle both dense and sparse input, with the latter converted to
      Compressed Sparse Rows (CSR) format automatically.
* To convert categorical features to integer codes, we can use the `OrdinalEncoder`.
    - Such integer representation can, however, not be used directly with all
      scikit-learn estimators, as these expect continuous input, and would
      interpret the categories as being ordered, which is often not desired.
* `OneHotEncoder` can be used to convert categorical  features into one-hot
  encoding and allows explicit specification of categories by using the
  `categories` parameter.
    - `handle_unknown='ignore'` is only supported for one-hot encoding
    - It is also possible to encode each column into `n_categories - 1` columns instead of `n_categories` columns by using the `drop` parameter. This parameter allows the user to specify a category for each feature to be dropped. This is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression (LinearRegression), since co-linearity would cause the covariance matrix to be non-invertible. When this paramenter is not None, `handle_unknown` must be set to `error`.
* One-hot encoded discretized (a.k,a. binning) features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models.
    - `KBinsDiscretizer` discretizes features into k bins. By default the output is one-hot encoded into a sparse matrix (See Encoding categorical features) and this can be configured with the `encode` parameter.
        * KBinsDiscretizer implements different binning strategies, which can be selected with the strategy parameter. The ‘uniform’ strategy uses constant-width bins. The ‘quantile’ strategy uses the quantiles values to have equally populated bins in each feature. The ‘kmeans’ strategy defines bins based on a k-means clustering procedure performed on each feature independently.
* Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution.
    - This is provided by the function `binarize` and transformer class
      `Binarizer`, which like in case of `Normalizer`, does nothing during `fit`
      as each sample is treated independently of others.
    - `Binarizer` is similar to the `KBinsDiscretizer` when k = 2, and when the bin edge is at the value of the parameter `threshold` of `Binarizer`.
* `PolynomialFeatures` allows one to get features'  higher-order and interaction
  terms.
    - Setting `interaction_only=True` gives only the interaction terms and not
      the other higher-order terms.
* You can implement a transformer from an arbitrary function with `FunctionTransformer`.

## Imputation of missing values

* One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. impute.SimpleImputer).
* By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. impute.IterativeImputer).
* The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located.
    - The SimpleImputer class also supports sparse matrices.
    - The SimpleImputer class also supports categorical data represented as
      string values or pandas categoricals when using the `'most_frequent'` or
      'constant' strategy.
*  The IterativeImputer class models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for `max_iter` imputation rounds. The results of the final imputation round are returned.
    - This estimator is still experimental for now: the predictions and the API might change without any deprecation cycle. To use it, you need to explicitly import `enable_iterative_imputer`.
    - Note that a call to the transform method of IterativeImputer is not
      allowed to change the number of samples. Therefore multiple imputations
      cannot be achieved by a single call to transform.
* The MissingIndicator transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative.

## Unsupervised dimensionality reduction

* If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps.
* Many of the Unsupervised learning methods implement a transform method that can be used to reduce the dimensionality.
* decomposition.PCA looks for a combination of features that capture well the variance of the original features. 
* The module `random_projection` provides several tools for data reduction by random projections.
* cluster.FeatureAgglomeration applies Hierarchical clustering to group together features that behave similarly.
    -  If features have very different scaling or statistical properties,
       cluster.FeatureAgglomeration may not be able to capture the links between
       related features.

## Random Projection

* The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.
* The main theoretical result behind the efficiency of random projection is the
  Johnson-Lindenstrauss lemma.
    - a small set of points in a high-dimensional space can be embedded into a
      space of much lower dimension in such a way that distances between the
      points are nearly preserved. The map used for the embedding is at least
      Lipschitz, and can even be taken to be an orthogonal projection.
    - more technically, for any `\epsilon` in (0,1), a set X of m points in R^N
      can be linearly mapped by a function f to R^n where `n > 8 ln(m)/
      \epsilon^2` such that the the distance between any two points u & v in the
      mapped space is within `\epsilon` percent of their original distance.
* Knowing only the number of samples, the `sklearn.random_projection.johnson_lindenstrauss_min_dim` estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection.
* The `sklearn.random_projection.GaussianRandomProjection` reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from normal N(0, 1/no. of components)
* The `sklearn.random_projection.SparseRandomProjection` reduces the dimensionality by projecting the original input space using a sparse random matrix.
    - Sparse random matrices are an alternative to dense Gaussian random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data.
    - The non-zero elements are drawn with a total probability of a parameter
      called "density", which is set to `1/\sqrt{n_features}`.

## Kernel approximation

* The advantage of using approximate explicit feature maps compared to the
  kernel trick, which makes use of feature maps implicitly, is that explicit
  mappings can be better suited for online learning and can significantly reduce
  the cost of learning with very large datasets. Standard kernelized SVMs do not
  scale well to large datasets, but using an approximate kernel map it is
  possible to use much more efficient linear SVMs. In particular, the
  combination of kernel map approximations with SGDClassifier can make
  non-linear learning on large datasets possible.

* The Nystroem method, as implemented in Nystroem is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default Nystroem uses the rbf kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter `n_components`.
* The RBFSampler constructs an approximate mapping for the radial basis function kernel, also known as Random Kitchen Sinks. This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM.
    - The mapping relies on a Monte Carlo approximation to the kernel values. The fit function performs the Monte Carlo sampling, whereas the transform method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the fit function.
    - The fit function takes two arguments: `n_components`, which is the target
      dimensionality of the feature transform, and gamma, the parameter of the
      RBF-kernel. A higher n_components will result in a better approximation of
      the kernel and will yield results more similar to those produced by a
      kernel SVM. Note that “fitting” the feature function does not actually
      depend on the data given to the fit function. Only the dimensionality of
      the data is used.
    - For a given value of `n_components` RBFSampler is often less accurate as Nystroem. RBFSampler is cheaper to compute, though, making use of larger feature spaces more efficient.
* The additive chi squared kernel is a kernel on histograms, often used in computer vision and is defined as:
`k(x,y) = \sum_i 2*x_i*y_i/(x_i+y_i)`, which is not exactly the same as `sklearn.metrics.additive_chi2_kernel`, but is preferable as it's always positive definite.
    - Since the kernel is additive, it is possible to treat all components  separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling.
    - The class AdditiveChi2Sampler implements this component wise deterministic sampling. Each component is sampled n times, yielding  2n+1 dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature,  is usually chosen to be 1 or 2.
    - The approximate feature map provided by AdditiveChi2Sampler can be combined with the approximate feature map provided by RBFSampler to yield an approximate feature map for the exponentiated chi squared kernel.
* The  skewed chi squared kernel, given by `k(x,y) = \prod_i 2 \sqrt(x_i+c)
  \sqrt(y_i+c)/ (x_i+y_i+2c)`, has properties that are similar to the
  exponentiated chi squared kernel often used in computer vision, but allows for
  a simple Monte Carlo approximation of the feature map.

## Pairwise metrics, Affinities and Kernels

* Distance metrics are functions such that d(a,b) < d(a,c) if objects a & b are
  considered "more similar" than a & c.
    - They satisfy the 3 axioms of distance: non-negativity, symmetry and
      triangle inequality
* Kernels are measures of similarity, i.e., s(a,b) > s(a,c) if a & b are "more
  similar" than a & c.
    - A kernel must be positive semi-definite.
* A distance metric and similarity kernel can be interconverted in various ways.
* The distances between the row vectors of X and the row vectors of Y can be evaluated using `pairwise_distances`.
    - If Y is omitted the pairwise distances of the row vectors of X are calculated.
* `pairwise.pairwise_kernels` can be used to calculate the kernel between X and Y using different kernel functions.
    - the following kernels are supported: `[‘additive_chi2’, ‘chi2’, ‘linear’, ‘poly’, ‘polynomial’, ‘rbf’,
‘laplacian’, ‘sigmoid’, ‘cosine’]`
* `cosine_similarity` computes the L2-normalized dot product of two vectors.
    - popular choice for computing document similarity using tf-idf vectors.
* `linear_kernel` computes the linear kernel, that is, a special case of `polynomial_kernel` with degree=1 and coef0=0 (homogeneous) as k(x,y) = x^T y
* `polynomial_kernel` computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors as `k(x,y) = (\gamma x^T y + c_0)^d`
    - Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions.
    - When used in machine learning algorithms, this allows to account for feature interaction.
* The chi-squared kernel is a very popular choice for training non-linear SVMs
  in computer vision applications given by: `k(x,y) = exp(-\gamma \sum_i (x[i] -
  y[i])^2 / (x[i] + y[i]) )`
    - The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.
    - The chi squared kernel is most commonly used on histograms (bags) of visual words.
* `sigmoid_kernel` computes the sigmoid kernel between two vectors as `k(x,y) =
  tanh(\gamma x^T y + c_0)`
* `rbf_kernel`: `k(x,y) = exp(-\gamma || x - y||^2)`
* `laplacian_kernel` is a variant on the radial basis kernel function defined as
  `k(x,y) = exp(-\gamma ||x-y||_1)`, and uses the Manhattan distance between the
  vectors.

## Transforming the prediction target

* `LabelBinarizer` is a utility class to help create a label indicator matrix from
  a list of multi-class labels.
    - For multiple labels per instance, use `MultiLabelBinarizer`
* `LabelEncoder` is a utility class to help normalize labels such that they contain only values between 0 and `n_classes-1`. 

# Dataset loading utilities

* There are three main kinds of dataset interfaces that can be used to get
  datasets:
    i. The dataset loaders. They can be used to load small standard datasets
    i. The dataset fetchers. They can be used to download and load larger dataset
    i. The dataset generation functions. They can be used to generate controlled synthetic datasets.
* Almost all of these function can return the output as a tuple containing only the data and the target, by setting the `return_X_y` parameter to True.

## Toy datasets

* scikit-learn comes with a few small standard datasets that do not require to download any file from some external website, including:
	- `load_boston([return_X_y])`:	Load and return the boston house-prices dataset (regression).
	- `load_iris([return_X_y])`:	Load and return the iris dataset (classification).
	- `load_diabetes([return_X_y])`:	Load and return the diabetes dataset (regression).
	- `load_digits([n_class, return_X_y])`:	Load and return the digits dataset (classification).
	- `load_linnerud([return_X_y])`:	Load and return the linnerud dataset (multivariate regression).
	- `load_wine([return_X_y])`:	Load and return the wine dataset (classification).
	- `load_breast_cancer([return_X_y])`:	Load and return the breast cancer wisconsin dataset (classification)

## Real-world datasets

* scikit-learn provides tools to load larger datasets, downloading them if
  necessary, including:
    - `fetch_olivetti_faces([data_home, shuffle, …])`:	Load the Olivetti faces data-set from AT&T (classification).
    - `fetch_20newsgroups([data_home, subset, …])`:	Load the filenames and data from the 20 newsgroups dataset (classification).
    - `fetch_20newsgroups_vectorized([subset, …])`:	Load the 20 newsgroups dataset and vectorize it into token counts (classification).
    - `fetch_lfw_people([data_home, funneled, …])`:	Load the Labeled Faces in the Wild (LFW) people dataset (classification).
    - `fetch_lfw_pairs([subset, data_home, …])`:	Load the Labeled Faces in the Wild (LFW) pairs (detect if two faces in pair belong to the same person) dataset (classification).
    - `fetch_covtype([data_home, …])`:	Load the forest covertype dataset (classification).
    - `fetch_rcv1([data_home, subset, …])`:	Load the RCV1 multilabel dataset (classification).
    - `fetch_kddcup99([subset, data_home, shuffle, …])`:	Load the kddcup99 dataset (classification).
    - `fetch_california_housing([data_home, …])`:	Load the California housing dataset (regression).

## Generated dstasets

* Both `make_blobs` and `make_classification` create multiclass datasets by allocating each class one or more normally-distributed clusters of points.
    - `make_blobs` provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering.
    - `make_classification` specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.
* `make_gaussian_quantiles` divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres.
* `make_hastie_10_2` generates a similar binary, 10-dimensional problem.
* `make_circles` and `make_moons` generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise.
    - They are useful for visualisation.
    - `make_circles` produces Gaussian data with a spherical decision boundary for binary classification, while `make_moons` produces two interleaving half circles.
* `make_multilabel_classification` generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics.
    - The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution.
    - Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words.
* For biclustering, the following generators are available in sklearn:
    - `make_biclusters(shape, n_clusters[, noise, …]`:)	Generate an array with constant block diagonal structure for biclustering.
    - `make_checkerboard(shape, n_clusters[, …])`:	Generate an array with block checkerboard structure for biclustering.
* `make_regression` produces regression targets as an optionally-sparse random linear combination of random features, with noise.
    - Its informative features may be uncorrelated, or low rank (few features account for most of the variance).
* ` make_sparse_uncorrelated` produces a target as a linear combination of four features with fixed coefficients.
* `make_friedman1` is related by polynomial and sine transforms; `make_friedman2` includes feature multiplication and reciprocation; and `make_friedman3` is similar with an arctan transformation on the target.
* For manifold learning, the following generators are available in sklearn:
    - `make_s_curve([n_samples, noise, random_state])`:	Generate an S curve dataset.
    - `make_swiss_roll([n_samples, noise, random_state])`:	Generate a swiss roll dataset.
* For decomposition, the following generators are available in sklearn:
    - `make_low_rank_matrix([n_samples, …]`:)	Generate a mostly low rank matrix with bell-shaped singular values
    - `make_sparse_coded_signal(n_samples, …[, …]`:)	Generate a signal as a sparse combination of dictionary elements.
    - `make_spd_matrix(n_dim[, random_state]`:)	Generate a random symmetric, positive-definite matrix.
    - `make_sparse_spd_matrix([dim, alpha, …]`:)	Generate a sparse symmetric definite positive matrix.

## Loading other datasets

### Sample images

* `load_sample_images()`:	Load sample images for image manipulation.
* `load_sample_image(image_name)`:	Load the numpy array of a single sample image

### Libsvm format

* scikit-learn includes utility functions, viz., `load_svmlight_file` for loading datasets in the svmlight / libsvm format. In this format, each line takes the form `<label> <feature-id>:<feature-value> <feature-id>:<feature-value> .... `.
    - This format is especially suitable for sparse datasets.
    - In this module, scipy sparse CSR matrices are used for X and numpy arrays are used for y

### OpenML

* The `sklearn.datasets` package is able to download datasets from the openml.org public repository using the function `sklearn.datasets.fetch_openml`.
* A dataset is uniquely specified by its `data_id`, but not necessarily by its name.
* Several different "versions" of a dataset with the same name can exist which can contain entirely different datasets.
* If a particular version of a dataset has been found to contain significant issues, it might be deactivated.
* Using a name to specify a dataset will yield the earliest version of a dataset that is still active.

### External databases

* Some recommended ways to load standard columnar data into a format usable by scikit-learn:
    - `pandas.io` provides tools to read data from common formats including CSV, Excel, JSON and SQL. DataFrames may also be constructed from lists of tuples or dicts. Pandas handles heterogeneous data smoothly and provides tools for manipulation and conversion into a numeric array suitable for scikit-learn.
    - `scipy.io` specializes in binary formats often used in scientific computing context such as .mat and .arff
    - `numpy/routines.io` for standard loading of columnar data into numpy arrays
    - scikit-learn’s `datasets.load_svmlight_file` for the svmlight or libSVM sparse format
    - scikit-learn’s `datasets.load_files` for directories of text files where the name of each directory is the name of each category and each file inside of each directory corresponds to one sample from that category
    - `skimage.io` or Imageio for loading images and videos into numpy arrays
    - `scipy.io.wavfile.read` for reading WAV files into a numpy array
* If you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format.

# Computing with scikit-learn

## Scaling using out-of-core learning

* Out-of-core (or “external memory”) learning is a technique used to learn from
  data that cannot fit in a computer’s main memory (RAM).
* 3 ways:
    i. streaming instances, e.g., a reader that yields instances from files on a
    hard drive, a database, from a network stream etc.
    i. extracting features, e.g., any relevant way to extract features among the
    different feature extraction methods supported by scikit-learn.
        - when working with data that needs vectorization and where the set of
          features or values is not known in advance, e.g., text classification
          where unknown terms are likely to be found during training, you must
          be careful.
        - the preferred way to do this is to use the so-called hashing trick as implemented by `sklearn.feature_extraction.FeatureHasher` for datasets with categorical variables represented as list of Python dicts or `sklearn.feature_extraction.text.HashingVectorizer` for text documents.
    i. incremental learning, e.g., using `partial_fit` method of estimators that
    support this method, which includes:
        - Classification:
            * `sklearn.naive_bayes.MultinomialNB`
            * `sklearn.naive_bayes.BernoulliNB`
            * `sklearn.linear_model.Perceptron`
            * `sklearn.linear_model.SGDClassifier`
            * `sklearn.linear_model.PassiveAggressiveClassifier`
            * `sklearn.neural_network.MLPClassifier`
        - Regression:
            * `sklearn.linear_model.SGDRegressor`
            * `sklearn.linear_model.PassiveAggressiveRegressor`
            * `sklearn.neural_network.MLPRegressor`
        - Clustering
            * `sklearn.cluster.MiniBatchKMeans`
            * `sklearn.cluster.Birch`
        - Decomposition / feature Extraction
            * `sklearn.decomposition.MiniBatchDictionaryLearning`
            * `sklearn.decomposition.IncrementalPCA`
            * `sklearn.decomposition.LatentDirichletAllocation`
        - Preprocessing
            * `sklearn.preprocessing.StandardScaler`
            * `sklearn.preprocessing.MinMaxScaler`
            * `sklearn.preprocessing.MaxAbsScaler`
        - For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first `partial_fit` call using the `classes=` parameter.
        - Another aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the Perceptron is still sensitive to badly labeled examples even after many examples whereas the SGD* and PassiveAggressive* families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time.

## Computational performance

* Prediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds).
* Latency is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this distribution (e.g. the 90 percentile).
* Prediction throughput is defined as the number of predictions the software can deliver in a given amount of time (e.g. in predictions per second).
* An important aspect of performance optimization is also that it can hurt prediction accuracy.
    - Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones.
* The main factors that influence the prediction latency are:
    - Number of features
    - Input data representation and sparsity
    - Model complexity
    - Feature extraction
    - A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode.
        * In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.).
* Scikit-learn does some validation on data that increases the overhead per call to predict and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable `SKLEARN_ASSUME_FINITE` to a non-empty string before importing scikit-learn, or configure it in Python with `sklearn.set_config`. For more control than these global settings, a `config_context` allows you to set this configuration within a specified context:
```
>>>
>>> import sklearn
>>> with sklearn.config_context(assume_finite=True):
...     pass  # do learning/prediction here with reduced validation
```
* Obviously when the number of features increases so does the memory consumption
  of each example.
    - For a matrix of M instances with N features, space complexity is O(MN).
    - From a computing perspective it also means that the number of basic
      operations (e.g., multiplications for vector-matrix products in linear
      models) increases too.
    - Overall you can expect the prediction time to increase at least linearly
      with the number of features (non-linear cases can happen depending on the
      global memory footprint and estimator).
* A non-zero value in a sparse (CSR or CSC) representation will only take on average one 32bit integer position + the 64 bit floating point value + an additional 32bit per row or column in the matrix.
    - Using sparse input on a dense (or sparse) linear model can speedup prediction by quite a bit as only the non zero valued features impact the dot product and thus the model predictions.
    - Calculation over a dense representation, however, may leverage highly optimised vector operations and multithreading in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input representation on a machine with many CPUs and an optimized BLAS implementation.
* Generally speaking, when model complexity increases, predictive power and
  latency are supposed to increase.
    - For `sklearn.linear_model` (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge & RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent.
    - For the sklearn.svm family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector.
    - For sklearn.ensemble of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees.
* Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries.
    - On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time.
    - In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application.
* As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries.
    - Basically, you ought to make sure that Numpy is built using an optimized BLAS / LAPACK library.
    - Not all models benefit from optimized BLAS and Lapack implementations.
    - For instance models based on (randomized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs (SVC, SVR, NuSVC, NuSVR).
    - On the other hand a linear model implemented with a BLAS DGEMM call (via numpy.dot) will typically benefit hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized BLAS.
* Where computations can be performed in fixed-memory chunks, sklearn attempts to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using sklearn.`set_config` or `config_context`.
* Model compression in scikit-learn only concerns linear models for the moment.
    - In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors).
    - It is generally a good idea to combine model sparsity with sparse input data representation.
```
clf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)
clf.fit(X_train, y_train).sparsify()
clf.predict(X_test)
```
* Model reshaping consists in selecting only a portion of the available features to fit a model.
    - In other words, if a model discards features during the learning phase we can then strip those from the input.
    - At the moment, reshaping needs to be performed manually in scikit-learn.
* Scikit-learn uses the joblib library to enable parallel computing inside its estimators.
    - by default, scikit-learn uses its embedded (vendored) version of joblib.
* `sklearn.set_config` controls the following behaviors:
    - `assume_finite`:	used to skip validation, which enables faster computations but may lead to segmentation faults if the data contains NaNs.
    - `working_memory`:	the optimal size of temporary arrays used by some algoritms.
* These environment variables should be set before importing scikit-learn:
    - `SKLEARN_SITE_JOBLIB`: When this environment variable is set to a non zero value, scikit-learn uses the site joblib rather than its vendored version. Consequently, joblib must be installed for scikit-learn to run. Note that using the site joblib is at your own risks: the versions of scikit-learn and joblib need to be compatible. Currently, joblib 0.11+ is supported. In addition, dumps from joblib.Memory might be incompatible, and you might loose some caches and have to redownload some datasets. Deprecated since version 0.21: As of version 0.21 this parameter has no effect, vendored joblib was removed and site joblib is always used.
    - `SKLEARN_ASSUME_FINITE`: Sets the default value for the `assume_finite` argument of `sklearn.set_config`.
    - `SKLEARN_WORKING_MEMORY`: Sets the default value for the Limiting Working Memory argument of `sklearn.set_config`.
    - `SKLEARN_SEED`:	Sets the seed of the global random generator when running the tests, for reproducibility.
    - `SKLEARN_SKIP_NETWORK_TESTS`: When this environment variable is set to a non zero value, the tests that need network access are skipped.

